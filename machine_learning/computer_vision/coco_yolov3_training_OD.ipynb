{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4036,
     "status": "ok",
     "timestamp": 1618726125132,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "EpvZnHe4OeLv",
    "outputId": "7e2ec383-95d5-48b3-a4af-f0e3f2a4cc65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount(\"/content/drive\")\n",
    "os.chdir(\"/content/drive/MyDrive/Libraries\")\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.layers import Layer, Dense, Flatten, Dropout, Concatenate, Add, Dot, Multiply, Reshape, Activation, BatchNormalization, SimpleRNNCell, RNN, SimpleRNN, LSTM, Embedding, Bidirectional, TimeDistributed, Conv1D, Conv2D, MaxPool1D, MaxPool2D, GlobalMaxPool1D, GlobalMaxPool2D, AveragePooling1D, AveragePooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adagrad\n",
    "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, CosineSimilarity\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.activations import linear, sigmoid, relu\n",
    "from tensorflow.keras.initializers import RandomNormal, glorot_uniform, he_uniform, Constant\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import colorsys\n",
    "import numpy as np\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 4026,
     "status": "ok",
     "timestamp": 1618726125133,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "gg4JnH5wpq-R",
    "outputId": "fdb65249-9e33-481b-8024-dbdb1f008d58"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir(\"/content/drive/MyDrive/CV\")\n",
    "\n",
    "input_size = 416\n",
    "batch_size = 4\n",
    "n_grids = [52, 26, 13]\n",
    "strides = [416//grid for grid in n_grids]\n",
    "anchors = [[[10, 13], [16, 30], [33, 23]], [[30, 61], [62, 45], [59, 119]], [[116, 90], [156, 198],\n",
    "                                                                             [373, 326]]]\n",
    "anchors = anchors/(np.array(strides).T[:, None, None])\n",
    "max_bbox_per_scale = 100\n",
    "\n",
    "init_lr = 1e-4\n",
    "fin_lr = 1e-6\n",
    "warmup_epochs = 2\n",
    "epochs = 30\n",
    "\n",
    "pref = \"/content/drive/My Drive/CV/racoon_data\"\n",
    "tr_annot_path = pref + \"/racoon_train.txt\"\n",
    "test_annot_path = pref + \"/racoon_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 4019,
     "status": "ok",
     "timestamp": 1618726125135,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "h_qnAiNGpoGI",
    "outputId": "6c38854d-f83d-4c77-d745-37091a8da785"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_giou(bbox1, bbox2):\n",
    "    # boxes : (x1, y1, x2, y2)\n",
    "    bbox1 = np.array(bbox1)\n",
    "    bbox2 = np.array(bbox2)\n",
    "\n",
    "    area_bbox1 = (bbox1[2] - bbox1[0])*(bbox1[3] - bbox1[1])\n",
    "    area_bbox2 = (bbox2[2] - bbox2[0])*(bbox2[3] - bbox2[1])\n",
    "\n",
    "    pt1_intersec = np.maximum(bbox1[:2], bbox2[:2])\n",
    "    pt2_intersec = np.minimum(bbox1[2:], bbox2[2:])\n",
    "    w_intersec, h_intersec = np.maximum(pt2_intersec - pt1_intersec, 0)\n",
    "    area_intersec = w_intersec*h_intersec\n",
    "\n",
    "    area_union = area_bbox1 + area_bbox2 - area_intersec\n",
    "\n",
    "    iou = np.maximum(area_intersec/area_union, np.finfo(np.float32).eps)\n",
    "\n",
    "    pt1_enclose = np.minimum(bbox1[:2], bbox2[:2])\n",
    "    pt2_enclose = np.maximum(bbox1[2:], bbox2[2:])\n",
    "    w_enclose, h_enclose = np.maximum(pt2_enclose - pt1_enclose, 0)\n",
    "    area_enclose = w_enclose*h_enclose\n",
    "\n",
    "    return iou - (area_enclose - area_union)/area_enclose\n",
    "\n",
    "def preprocess_image(img, gt_boxes=None):\n",
    "    tar_h = input_size\n",
    "    tar_w = input_size\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    scale = min(tar_h/h, tar_w/w)\n",
    "\n",
    "    new_w, new_h = int(scale*w), int(scale*h)\n",
    "    img_resized = cv2.resize(img, dsize=(new_w, new_h))\n",
    "\n",
    "    img_paded = np.full(shape=(tar_h, tar_w, 3), fill_value=128.)\n",
    "    pad_w, pad_h = (tar_w - new_w)//2, (tar_h - new_h)//2\n",
    "    img_paded[pad_h:pad_h+new_h, pad_w:pad_w+new_w, :] = img_resized\n",
    "    img_paded = img_paded/255.\n",
    "\n",
    "    if gt_boxes is None:\n",
    "        return img_paded\n",
    "    else:\n",
    "        # (x1, y1, x2, y2)\n",
    "        gt_boxes[:, (0, 2)] = gt_boxes[:, (0, 2)]*scale + pad_w\n",
    "        gt_boxes[:, (1, 3)] = gt_boxes[:, (1, 3)]*scale + pad_h\n",
    "        return img_paded, gt_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 4000,
     "status": "ok",
     "timestamp": 1618726125136,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "bVciagx0Xj6S",
    "outputId": "6c67c573-a15c-46f3-d6a2-30aff2aa4b2f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BatchNormalization(BatchNormalization):\n",
    "    # When `layer.trainable=False` is set(inference mode), the layer is frozen and will use stored\n",
    "    # moving `var` and `mean` and both `gamma` and `beta` will not be updated.\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            training = tf.constant(False)\n",
    "        training = tf.logical_and(training, self.trainable)\n",
    "        return super().call(x, training)\n",
    "\n",
    "def convolutional(x, filters, kernel_size, downsample=False, activate=True, bn=True):\n",
    "    if downsample == False:\n",
    "        strides = 1\n",
    "        padding = \"same\"\n",
    "        z = x\n",
    "    elif downsample == True:\n",
    "        # top and left padding\n",
    "        # shape: (batch_size, h, w, channels) -> (batch_size, h+1, w+1, channels)\n",
    "        # the image size of the output is half the input.\n",
    "        z = ZeroPadding2D(padding=((1, 0), (1, 0)))(x)\n",
    "        strides = 2\n",
    "        padding = \"valid\"\n",
    "\n",
    "    z = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "                padding=padding, use_bias=not bn, kernel_regularizer=l2(0.0005),\n",
    "                kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                bias_initializer=tf.constant_initializer(0.))(z)\n",
    "    if bn == True:\n",
    "        z = BatchNormalization()(z)\n",
    "    if activate == True:\n",
    "        z = LeakyReLU(alpha=0.1)(z)\n",
    "\n",
    "    return z\n",
    "\n",
    "def residual_block(x, filters):\n",
    "    z = convolutional(x, filters=filters[0], kernel_size=1)\n",
    "    z = convolutional(z, filters=filters[1], kernel_size=3)\n",
    "\n",
    "    return Add()([x, z])\n",
    "\n",
    "def upsample(x):\n",
    "    return tf.image.resize(images=x, size=(x.shape[1]*2, x.shape[2]*2), method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5113,
     "status": "ok",
     "timestamp": 1618726126261,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "OCQiMdKTng3J",
    "outputId": "f67cdfe3-e4a7-4dfb-9a78-e02063a41abb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pred: pred, conv:, label: gt\n",
    "def compute_loss(pred, conv, label, bboxes, i=0):\n",
    "    # batch_size  = tf.shape(conv)[0]\n",
    "    # output_size = n_grids[i]\n",
    "    # (batch_size, n_grids[i], n_grids[i], 3*(5 + n_clss))\n",
    "    # -> (batch_size, n_grids[i], n_grids[i], 3, 5 + n_clss)\n",
    "    conv = tf.reshape(conv, (batch_size, n_grids[i], n_grids[i], 3, 5 + n_clss))\n",
    "    conv_raw_conf = conv[:, :, :, :, 4]\n",
    "    conv_raw_prob = conv[:, :, :, :, 5:]\n",
    "\n",
    "    pred_xywh     = pred[:, :, :, :, 0:4]\n",
    "    pred_conf     = pred[:, :, :, :, 4] #시그머이드 반영\n",
    "\n",
    "    label_xywh    = label[:, :, :, :, 0:4]\n",
    "    label_conf  = label[:, :, :, :, 4] #있으면 1 아니면 0\n",
    "    label_prob    = label[:, :, :, :, 5:] #해당 클래스 있는것만 1 나머지 0\n",
    "\n",
    "    # print(pred_xywh.shape, label_xywh[..., None].shape)\n",
    "    # giou = compute_giou(pred_xywh, label_xywh)[..., None]\n",
    "    giou = np.array([compute_giou(pred_xywh, xywh) for xywh in label_xywh])[..., None]\n",
    "    # input_size = tf.cast(input_size, tf.float32)\n",
    "\n",
    "    bbox_loss_scale = 2.0 - 1.0*label_xywh[:, :, :, :, 2]*label_xywh[:, :, :, :, 3]/(input_size**2)\n",
    "    giou_loss = label_conf*bbox_loss_scale*(1 - giou)\n",
    "\n",
    "    # iou = bbox_iou(pred_xywh[:, :, :, :, None, :], bboxes[:, None, None, None, :, :])\n",
    "    iou = np.array([compute_giou(pred_xywh[:, :, :, :, None, :], bbox)\\\n",
    "                    for bbox in bboxes[:, None, None, None, :, :]])\n",
    "    max_iou = tf.reduce_max(iou, axis=-1)[..., None]\n",
    "\n",
    "    respond_bgd = (1.0 - label_conf)*tf.cast(max_iou < iou_thrsd, tf.float32)\n",
    "# RETINANET\n",
    "    # conf_focal = tf.pow(label_conf - pred_conf, 2)\n",
    "    conf_focal = (label_conf - pred_conf)**2\n",
    "\n",
    "    conf_loss = conf_focal*(label_conf*tf.nn.sigmoid_cross_entropy_with_logits(labels=label_conf,\n",
    "                                                                                 logits=conv_raw_conf)\n",
    "            +respond_bgd*tf.nn.sigmoid_cross_entropy_with_logits(labels=label_conf, logits=conv_raw_conf))\n",
    "\n",
    "    prob_loss = label_conf*tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n",
    "\n",
    "    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1, 2, 3, 4]))\n",
    "    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1, 2, 3, 4]))\n",
    "    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1, 2, 3, 4]))\n",
    "\n",
    "    return giou_loss, conf_loss, prob_loss\n",
    "\n",
    "idx2cls = {}\n",
    "with open(\"/content/drive/My Drive/Computer Vision/model_data/coco.names\", \"r\") as data:\n",
    "    for idx, cls in enumerate(data):\n",
    "        idx2cls[idx] = cls.strip(\"\\n\")\n",
    "n_clss = len(idx2cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5105,
     "status": "ok",
     "timestamp": 1618726126262,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "8L0X8LQxXvny",
    "outputId": "8b38e25e-cfb9-475e-fcf1-eb18e2883da4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def YOLOv3(training=False):\n",
    "    input_size = 416\n",
    "    channels = 3\n",
    "    inputs  = Input([input_size, input_size, channels])\n",
    "\n",
    "    # Darknet 53 from here(totally 75 layers).\n",
    "    z = convolutional(inputs, filters=32, kernel_size=3)\n",
    "    z = convolutional(z, filters=64, kernel_size=3, downsample=True)\n",
    "    for _ in range(1):\n",
    "        z = residual_block(z, filters=[32, 64])\n",
    "    z = convolutional(z, filters=128, kernel_size=3, downsample=True)\n",
    "    for _ in range(2):\n",
    "        z = residual_block(z, filters=[64, 128])\n",
    "    z = convolutional(z, filters=256, kernel_size=3, downsample=True)\n",
    "    for _ in range(8):\n",
    "        z = residual_block(z, filters=[128, 256])\n",
    "    route1 = z\n",
    "\n",
    "    z = convolutional(z, filters=512, kernel_size=3, downsample=True)\n",
    "    for _ in range(8):\n",
    "        z = residual_block(z, filters=[256, 512])\n",
    "    route2 = z\n",
    "\n",
    "    z = convolutional(z, filters=1024, kernel_size=3, downsample=True)\n",
    "    for _ in range(4):\n",
    "        z = residual_block(z, filters=[512, 1024])\n",
    "\n",
    "    # YOLO v3 from here(totally 31 layers).\n",
    "    z = convolutional(z, filters=512, kernel_size=1)\n",
    "    z = convolutional(z, filters=1024, kernel_size=3)\n",
    "    z = convolutional(z, filters=512, kernel_size=1)\n",
    "    z = convolutional(z, filters=1024, kernel_size=3)\n",
    "    z = convolutional(z, filters=512, kernel_size=1)\n",
    "\n",
    "    conv_lobj_branch = convolutional(z, filters=1024, kernel_size=3)\n",
    "    # (batch_size, 13, 13, 3*(n_clss + 5))\n",
    "    conv_lbbox = convolutional(conv_lobj_branch, filters=3*(n_clss + 5), kernel_size=1,\n",
    "                                activate=False, bn=False)\n",
    "\n",
    "    z = convolutional(z, filters=256, kernel_size=1)\n",
    "    z = upsample(z)\n",
    "\n",
    "    z = Concatenate(axis=-1)([z, route2])\n",
    "\n",
    "    z = convolutional(z, filters=256, kernel_size=1)\n",
    "    z = convolutional(z, filters=512, kernel_size=3)\n",
    "    z = convolutional(z, filters=256, kernel_size=1)\n",
    "    z = convolutional(z, filters=512, kernel_size=3)\n",
    "    z = convolutional(z, filters=256, kernel_size=1)\n",
    "\n",
    "    conv_mobj_branch = convolutional(z, filters=512, kernel_size=3)\n",
    "    # (batch_size, 26, 26, 3*(n_clss + 5))\n",
    "    conv_mbbox = convolutional(conv_mobj_branch, filters=3*(n_clss + 5), kernel_size=1,\n",
    "                                activate=False, bn=False)\n",
    "\n",
    "    z = convolutional(z, filters=128, kernel_size=1)\n",
    "    z = upsample(z)\n",
    "\n",
    "    z = Concatenate(axis=-1)([z, route1])\n",
    "\n",
    "    z = convolutional(z, filters=128, kernel_size=1)\n",
    "    z = convolutional(z, filters=256, kernel_size=3)\n",
    "    z = convolutional(z, filters=128, kernel_size=1)\n",
    "    z = convolutional(z, filters=256, kernel_size=3)\n",
    "    z = convolutional(z, filters=128, kernel_size=1)\n",
    "\n",
    "    conv_sobj_branch = convolutional(z, filters=256, kernel_size=3)\n",
    "    # (batch_size, 52, 52, 3*(n_clss + 5))\n",
    "    conv_sbbox = convolutional(conv_sobj_branch, filters=3*(n_clss + 5), kernel_size=1,\n",
    "                                activate=False, bn=False)\n",
    "\n",
    "    outputs = []\n",
    "    for i, conv_bbox in enumerate([conv_sbbox, conv_mbbox, conv_lbbox]):\n",
    "        if training == True:\n",
    "            outputs.append(conv_bbox)\n",
    "        # (batch_size, output_size, output_size, 3*(5 + n_clss)\n",
    "        # batch_size = tf.shape(conv_bbox)[0]\n",
    "        output_size = n_grids[i]\n",
    "        conv_bbox = tf.reshape(conv_bbox, shape=(batch_size, output_size, output_size, 3, 5 + n_clss))\n",
    "        delta_xy = conv_bbox[:, :, :, :, 0:2]   \n",
    "        delta_wh = conv_bbox[:, :, :, :, 2:4]\n",
    "        conf = conv_bbox[:, :, :, :, 4:5]\n",
    "        probs = conv_bbox[:, :, :, :, 5: ] \n",
    "\n",
    "        y = tf.range(output_size, dtype=tf.int32)\n",
    "        y = tf.expand_dims(y, -1)\n",
    "        y = tf.tile(y, [1, output_size])\n",
    "        x = tf.range(output_size,dtype=tf.int32)\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        x = tf.tile(x, [output_size, 1])\n",
    "        xy_grid = tf.concat([x[:, :, None], y[:, :, None]], axis=-1)\n",
    "        # (output_size, output_size, 2) -> (batch_size, output_size, output_size, 3, 2)\n",
    "        xy_grid = tf.tile(xy_grid[None, :, :, None, :], [batch_size, 1, 1, 3, 1])\n",
    "        xy_grid = tf.cast(xy_grid, tf.float32)\n",
    "\n",
    "        # The center of the predicted bboxes in the original 416x416 image space.\n",
    "        xy = (tf.math.sigmoid(delta_xy) + xy_grid)*strides[i]\n",
    "        wh = (tf.math.exp(delta_wh)*anchors[i])*strides[i]\n",
    "        # xywh = tf.concat([xy, wh], axis=-1)\n",
    "        conf = tf.math.sigmoid(conf)\n",
    "        probs = tf.math.sigmoid(probs)\n",
    "                \n",
    "        # [(batch_size, 52, 52, 3, 85), (batch_size, 26, 26, 3 85), (batch_size, 13, 13, 3, 85)]\n",
    "        outputs.append(tf.concat([xy, wh, conf, probs], axis=-1))\n",
    "        # outputs.append(tf.concat([xywh, conf, probs], axis=-1))\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 7245,
     "status": "ok",
     "timestamp": 1618726128412,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "CQ1zG7qH8XQV",
    "outputId": "ccdf4dcc-507a-49f2-a435-75fc69cb5d65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Dataset(object):\n",
    "    # Dataset preprocess implementation\n",
    "    def __init__(self, dataset_type):\n",
    "        # (x, y, w, h)\n",
    "        self.annot_path = tr_annot_path if dataset_type == \"train\" else test_annot_path\n",
    "        self.data_aug = True if dataset_type == \"train\" else False\n",
    "\n",
    "        with open(tr_annot_path, \"r\") as f:\n",
    "            txt = f.readlines()\n",
    "            annots = [line.strip() for line in txt if len(line.strip().split()[1:]) != 0]\n",
    "            random.shuffle(annots)\n",
    "        self.final_annots = list()\n",
    "        for annot in annots:\n",
    "            img_path, bbox = annot.split()\n",
    "            self.final_annots.append([img_path, [bbox]])\n",
    "\n",
    "        self.n_samples = len(self.final_annots)\n",
    "        self.n_batchs = int(np.ceil(self.n_samples/batch_size))\n",
    "        self.batch_count = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # self.tr_input_size = random.choice([self.tr_input_sizes])\n",
    "\n",
    "        batch_img = np.zeros((batch_size, 416, 416, 3), dtype=np.float32)\n",
    "\n",
    "        batch_label_sbbox = np.zeros((batch_size, n_grids[0], n_grids[0],\n",
    "                                        3, 5 + n_clss), dtype=np.float32)\n",
    "        batch_label_mbbox = np.zeros((batch_size, n_grids[1], n_grids[1],\n",
    "                                        3, 5 + n_clss), dtype=np.float32)\n",
    "        batch_label_lbbox = np.zeros((batch_size, n_grids[2], n_grids[2],\n",
    "                                        3, 5 + n_clss), dtype=np.float32)\n",
    "\n",
    "        batch_sbboxes = np.zeros((batch_size, max_bbox_per_scale, 4), dtype=np.float32)\n",
    "        batch_mbboxes = np.zeros((batch_size, max_bbox_per_scale, 4), dtype=np.float32)\n",
    "        batch_lbboxes = np.zeros((batch_size, max_bbox_per_scale, 4), dtype=np.float32)\n",
    "\n",
    "        num = 0\n",
    "        if self.batch_count < self.n_batchs:\n",
    "            while num < batch_size:\n",
    "                idx = self.batch_count*batch_size + num\n",
    "                if idx >= self.n_samples:\n",
    "                    idx -= self.n_samples\n",
    "                annot = self.final_annots[idx]\n",
    "                # Parse annotation\n",
    "                img = cv2.imread(pref + annot[0][1:])\n",
    "                bboxes = np.array([list(map(int, box.split(','))) for box in annot[1]])\n",
    "                # print(bboxes)\n",
    "                if self.data_aug:\n",
    "                    img, bboxes = self.random_horizontal_flip(img, bboxes)\n",
    "                    img, bboxes = self.random_crop(img, bboxes)\n",
    "                    img, bboxes = self.random_translate(img, bboxes)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img, bboxes = preprocess_image(img=img, gt_boxes=bboxes)\n",
    "\n",
    "                # Preprocess GT bboxes.\n",
    "                # def preprocess_true_boxes(self, bboxes):\n",
    "                label = [np.zeros((n_grids[i], n_grids[i], 3,\n",
    "                                5 + n_clss)) for i in range(3)]\n",
    "                bboxes_xywh = [np.zeros((max_bbox_per_scale, 4)) for _ in range(3)]\n",
    "                bbox_count = np.zeros((3,))\n",
    "\n",
    "                for bbox in bboxes:\n",
    "                    bbox_coor = bbox[:4]\n",
    "                    bbox_class_ind = bbox[4]\n",
    "\n",
    "                    onehot = np.zeros(n_clss, dtype=np.float)\n",
    "                    onehot[bbox_class_ind] = 1.0\n",
    "                    uniform_distribution = np.full(n_clss, 1.0 / n_clss)\n",
    "                    deta = 0.01\n",
    "                    smooth_onehot = onehot*(1 - deta) + deta*uniform_distribution\n",
    "\n",
    "                    bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2])*0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1)\n",
    "                    bbox_xywh_scaled = 1.0*bbox_xywh[None, :] / strides[:, None]\n",
    "\n",
    "                    iou = []\n",
    "                    exist_positive = False\n",
    "                    for i in range(3):\n",
    "                        anchors_xywh = np.zeros((3, 4))\n",
    "                        anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n",
    "                        anchors_xywh[:, 2:4] = anchors[i]\n",
    "\n",
    "                        # iou_scale = bbox_iou(bbox_xywh_scaled[i][None, :], anchors_xywh)\n",
    "                        # iou_scale = compute_giou(bbox_xywh_scaled[i][None, :], anchors_xywh)\n",
    "                        iou_scale = np.array([compute_giou(bbox_xywh_scaled[i][None, :][0], anchor_xywh)\\\n",
    "                                                 for anchor_xywh in anchors_xywh])\n",
    "                        iou.append(iou_scale)\n",
    "                        iou_mask = iou_scale > 0.3\n",
    "\n",
    "                        if np.any(iou_mask):\n",
    "                            xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)\n",
    "\n",
    "                            label[i][yind, xind, iou_mask, :] = 0\n",
    "                            label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\n",
    "                            label[i][yind, xind, iou_mask, 4:5] = 1.0\n",
    "                            label[i][yind, xind, iou_mask, 5:] = smooth_onehot\n",
    "\n",
    "                            bbox_ind = int(bbox_count[i] % max_bbox_per_scale)\n",
    "                            bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n",
    "                            bbox_count[i] += 1\n",
    "\n",
    "                            exist_positive = True\n",
    "\n",
    "                    if not exist_positive:\n",
    "                        best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n",
    "                        best_detect = int(best_anchor_ind / 3)\n",
    "                        best_anchor = int(best_anchor_ind % 3)\n",
    "                        xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)\n",
    "\n",
    "                        label[best_detect][yind, xind, best_anchor, :] = 0\n",
    "                        label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n",
    "                        label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n",
    "                        label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot\n",
    "                        \n",
    "                        bbox_ind = int(bbox_count[best_detect] % max_bbox_per_scale)\n",
    "                        bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n",
    "                        bbox_count[best_detect] += 1\n",
    "\n",
    "                label_sbbox, label_mbbox, label_lbbox = label\n",
    "                sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
    "                    # return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes\n",
    "\n",
    "\n",
    "                # label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes =\\\n",
    "                # self.preprocess_true_boxes(bboxes)\n",
    "\n",
    "                batch_img[num, :, :, :] = img\n",
    "                batch_label_sbbox[num, :, :, :, :] = label_sbbox\n",
    "                batch_label_mbbox[num, :, :, :, :] = label_mbbox\n",
    "                batch_label_lbbox[num, :, :, :, :] = label_lbbox\n",
    "                batch_sbboxes[num, :, :] = sbboxes\n",
    "                batch_mbboxes[num, :, :] = mbboxes\n",
    "                batch_lbboxes[num, :, :] = lbboxes\n",
    "                num += 1\n",
    "            self.batch_count += 1\n",
    "            batch_smaller_target = batch_label_sbbox, batch_sbboxes\n",
    "            batch_medium_target  = batch_label_mbbox, batch_mbboxes\n",
    "            batch_larger_target  = batch_label_lbbox, batch_lbboxes\n",
    "\n",
    "            return batch_img, (batch_smaller_target, batch_medium_target, batch_larger_target)\n",
    "        else:\n",
    "            self.batch_count = 0\n",
    "            np.random.shuffle(self.final_annots)\n",
    "            raise StopIteration\n",
    "\n",
    "    def random_horizontal_flip(self, img, bboxes):\n",
    "        # With 50% probability.\n",
    "        if random.random() < 0.5:\n",
    "            _, w, _ = img.shape\n",
    "            img = img[:, ::-1, :]\n",
    "            bboxes[:, [0, 2]] = w - bboxes[:, [2, 0]]\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "    def random_crop(self, img, bboxes):\n",
    "        if random.random() < 0.5:\n",
    "            h, w, _ = img.shape\n",
    "            # bboxes: (x1, y1, x2, y2)\n",
    "            # cv2_imshow(img)\n",
    "            # print(bboxes)\n",
    "            enclose = np.concatenate([np.min(bboxes[:, 0:2], axis=0),\n",
    "                                      np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n",
    "            \n",
    "            max_l_trans = enclose[0]\n",
    "            max_u_trans = enclose[1]\n",
    "            max_r_trans = w - enclose[2]\n",
    "            max_d_trans = h - enclose[3]\n",
    "\n",
    "            crop_xmin = max(0, int(enclose[0] - random.uniform(0, max_l_trans)))\n",
    "            crop_ymin = max(0, int(enclose[1] - random.uniform(0, max_u_trans)))\n",
    "            crop_xmax = max(w, int(enclose[2] + random.uniform(0, max_r_trans)))\n",
    "            crop_ymax = max(h, int(enclose[3] + random.uniform(0, max_d_trans)))\n",
    "\n",
    "            img = img[crop_ymin:crop_ymax, crop_xmin:crop_xmax, :]\n",
    "\n",
    "            bboxes[:, (0, 2)] = bboxes[:, (0, 2)] - crop_xmin\n",
    "            bboxes[:, (1, 3)] = bboxes[:, (1, 3)] - crop_ymin\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "    def random_translate(self, img, bboxes):\n",
    "        if random.random() < 0.5:\n",
    "            h, w, _ = img.shape\n",
    "            enclose = np.concatenate([np.min(bboxes[:, 0:2], axis=0),\n",
    "                                      np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n",
    "\n",
    "            max_l_trans = enclose[0]\n",
    "            max_u_trans = enclose[1]\n",
    "            max_r_trans = w - enclose[2]\n",
    "            max_d_trans = h - enclose[3]\n",
    "\n",
    "            tx = random.uniform(-(max_l_trans - 1), (max_r_trans - 1))\n",
    "            ty = random.uniform(-(max_u_trans - 1), (max_d_trans - 1))\n",
    "\n",
    "            img = cv2.warpAffine(src=img, M=np.array([[1, 0, tx], [0, 1, ty]]), dsize=(w, h))\n",
    "\n",
    "            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx\n",
    "            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batchs\n",
    "\n",
    "trainset = Dataset(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 10916,
     "status": "ok",
     "timestamp": 1618726132095,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "eMp-XBHSXicX",
    "outputId": "004d16d6-db0f-4ab7-aa0f-8b10357ff339"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "global train_from_checkpoint\n",
    "\n",
    "save_best_only = True\n",
    "# Saves all the best validated checkpoints in training process.(This may require a lot of disk spaces.)\n",
    "save_checkpoints = False\n",
    "\n",
    "trainset = Dataset(\"train\")\n",
    "testset = Dataset(\"test\")\n",
    "\n",
    "steps_per_epoch = len(trainset)\n",
    "global_steps = tf.Variable(1, trainable=False, dtype=tf.int64)\n",
    "warmup_steps = warmup_epochs*steps_per_epoch\n",
    "tot_steps = epochs*steps_per_epoch\n",
    "\n",
    "model = YOLOv3(training=True)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "executionInfo": {
     "elapsed": 12052,
     "status": "error",
     "timestamp": 1618726133239,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "hF1xPglTzkLk",
    "outputId": "bd686778-a234-42f7-f3ba-0308c6b86743"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bd4c6c2986a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;31m# should be large at start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mpred_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a854ac9643a0>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mbbox_xywh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_coor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbbox_coor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_coor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbox_coor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0mbbox_xywh_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbbox_xywh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "trainset = Dataset(\"train\")\n",
    "transfer_learning = False\n",
    "if transfer_learning:\n",
    "    # Transfer learning from Darknet 53 weights.\n",
    "    # Resets all state generated by Keras.\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    with open(\"/content/drive/My Drive/Computer Vision/model_data/yolov3.weights\", \"rb\") as f:\n",
    "        _, _, _, _, _ = np.fromfile(f, dtype=np.int32, count=5)\n",
    "\n",
    "        j = 0\n",
    "        for i in range(75):\n",
    "            conv_layer = model.get_layer(\"conv2d\" if i == 0 else f\"conv2d_{i}\")\n",
    "            filters = conv_layer.filters\n",
    "            kernel_size = conv_layer.kernel_size[0]\n",
    "            in_dim = conv_layer.input_shape[-1]\n",
    "\n",
    "            if i not in [58, 66, 74]:\n",
    "                # order: [beta, gamma, mean, variance](darknet) -> [gamma, beta, mean, variance](tf)\n",
    "                bn_weights = np.fromfile(f, dtype=np.float32, count=4*filters).reshape((4, filters))[[1, 0, 2, 3]]\n",
    "                bn_layer = model.get_layer(\"batch_normalization\" if j == 0 else f\"batch_normalization_{j}\")\n",
    "                j += 1\n",
    "            else:\n",
    "                conv_bias = np.fromfile(f, dtype=np.float32, count=filters)\n",
    "\n",
    "            conv_shape = (filters, in_dim, kernel_size, kernel_size)\n",
    "            # shape: (out_dim, in_dim, h, w) -> (h, w, in_dim, out_dim)\n",
    "            conv_weights = np.fromfile(f, dtype=np.float32, count=np.prod(conv_shape)).reshape(conv_shape)\\\n",
    "            .transpose((2, 3, 1, 0))\n",
    "\n",
    "            if i not in [58, 66, 74]:\n",
    "                conv_layer.set_weights([conv_weights])\n",
    "                bn_layer.set_weights(bn_weights)\n",
    "            else:\n",
    "                conv_layer.set_weights([conv_weights, conv_bias])\n",
    "\n",
    "# if train_from_checkpoint:\n",
    "#     model.load_weights(\"./checkpoints/yolov3_custom\")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "best_val_loss = 1000 # should be large at start\n",
    "for epoch in range(epochs):\n",
    "    for img_data, target in trainset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_result = model(img_data, training=True)\n",
    "            giou_loss = conf_loss = prob_loss=0\n",
    "            # optimizing process\n",
    "            for i in range(3):\n",
    "                conv, pred = pred_result[i*2], pred_result[i*2 + 1]\n",
    "                loss_items = compute_loss(pred, conv,*target[i], i)\n",
    "                giou_loss += loss_items[0]\n",
    "                conf_loss += loss_items[1]\n",
    "                prob_loss += loss_items[2]\n",
    "            tot_loss = giou_loss + conf_loss + prob_loss\n",
    "            gras = tape.gradient(tot_loss, yolo.trainable_variables)\n",
    "            opt.apply_gradients(zip(grads, yolo.trainable_variables))\n",
    "\n",
    "            # update learning rate\n",
    "            global_steps.assign_add(1)\n",
    "            if global_steps < warmup_steps:# and not TRAIN_TRANSFER:\n",
    "                lr = global_steps / warmup_steps*init_lr\n",
    "            else:\n",
    "                lr = fin_lr + 0.5*(init_lr - fin_lr)*(\n",
    "                    (1 + tf.cos((global_steps - warmup_steps) / (tot_steps - warmup_steps)*np.pi)))\n",
    "            optimizer.lr.assign(lr.numpy())\n",
    "\n",
    "        results = global_steps.numpy(), opt.lr.numpy(), giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), tot_loss.numpy()\n",
    "        cur_step = results[0]%steps_per_epoch\n",
    "        print(f\"epoch:{epoch:2.0f} step:{cur_step:5.0f}/{steps_per_epoch}, lr:{results[1]:.6f},\\\n",
    "        giou_loss:{results[2]:7.2f}, conf_loss:{results[3]:7.2f}, prob_loss:{results[4]:7.2f},\\\n",
    "        tot_loss:{results[5]:7.2f}\")\n",
    "\n",
    "    count, giou_val, conf_val, prob_val, tot_val = 0., 0, 0, 0, 0\n",
    "    for img_data, target in testset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_result = model(img_data, training=False)\n",
    "            giou_loss=conf_loss=prob_loss=0\n",
    "\n",
    "            # optimizing process\n",
    "            for i in range(3):\n",
    "                conv, pred = pred_result[i*2], pred_result[i*2+1]\n",
    "                loss_items = compute_loss(pred, conv,*target[i], i)\n",
    "                giou_loss += loss_items[0]\n",
    "                conf_loss += loss_items[1]\n",
    "                prob_loss += loss_items[2]\n",
    "            tot_loss = giou_loss + conf_loss + prob_loss\n",
    "\n",
    "        results = giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), tot_loss.numpy()\n",
    "        count += 1\n",
    "        giou_val += results[0]\n",
    "        conf_val += results[1]\n",
    "        prob_val += results[2]\n",
    "        tot_val += results[3]\n",
    "        \n",
    "    print(f\"\\n\\ngiou_val_loss:{giou_val/count:7.2f}, conf_val_loss:{conf_val/count:7.2f},\\\n",
    "    prob_val_loss:{prob_val/count:7.2f}, tot_val_loss:{tot_val/count:7.2f}\\n\\n.\")\n",
    "\n",
    "    if save_best_only and best_val_loss > tot_val/count: \n",
    "        model.save_weights(\"/content/drive/My Drive/Computer Vision/checkpoints/yolov3_custom\")\n",
    "        best_val_loss = tot_val/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12040,
     "status": "aborted",
     "timestamp": 1618726133237,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "9S33GwqLjbpY"
   },
   "outputs": [],
   "source": [
    "YOLOv3(training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12038,
     "status": "aborted",
     "timestamp": 1618726133238,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "MKS9tanWXjI4"
   },
   "outputs": [],
   "source": [
    "ID = random.randint(0, 200)\n",
    "# model.load_weights(\"./checkpoints/yolov3_custom\")\n",
    "ori_img = cv2.imread(\"/content/drive/My Drive/Computer Vision/Allhydrants-1920x1080-ca61f9ea607efb2f02f1ef97b781ee0f.jpg\")\n",
    "\n",
    "img_paded = preprocess_image(img=ori_img)\n",
    "# Add a dimension for batch size.\n",
    "img_paded = img_paded[None, ...]\n",
    "\n",
    "pred_bboxes = model.predict(img_paded)\n",
    "pred_bboxes = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bboxes]\n",
    "# [(52*52*3, 85), (26*26*3, 85), (13*13*3, 85)] -> ((52*52*3 + 26*26*3 + 13*13*3), 85)\n",
    "pred_bboxes = tf.concat(pred_bboxes, axis=0)\n",
    "\n",
    "coors = pred_bboxes[:, :4]\n",
    "confs = pred_bboxes[:, 4]\n",
    "probs = pred_bboxes[:, 5:]\n",
    "\n",
    "# shape: (x, y, w, h) -> (x1, y1, x2, y2)\n",
    "coors = np.concatenate([coors[:, :2] - coors[:, 2:]*0.5,\n",
    "                        coors[:, :2] + coors[:, 2:]*0.5], axis=-1)\n",
    "\n",
    "# (x1, y1, x2, y2) -> (x1_ori, y1_ori, x2_ori, y2_ori)\n",
    "ori_h, ori_w = ori_img.shape[:2]\n",
    "resize_ratio = min(input_size/ori_w, input_size/ori_h)\n",
    "\n",
    "pad_w = (input_size - resize_ratio*ori_w)/2\n",
    "pad_h = (input_size - resize_ratio*ori_h)/2\n",
    "\n",
    "coors[:, (0, 2)] = (coors[:, (0, 2)] - pad_w)/resize_ratio\n",
    "coors[:, (1, 3)] = (coors[:, (1, 3)] - pad_h)/resize_ratio\n",
    "\n",
    "# Discard bboxes with larger (x2, y2) than (x1, y1).\n",
    "coors[:, :4] = np.concatenate([np.maximum(coors[:, (0, 1)], 0),\n",
    "                            np.minimum(coors[:, (2, 3)], [ori_w-1, ori_h-1])], axis=-1)\n",
    "coors[np.logical_or((coors[:, 0] > coors[:, 2]), (coors[:, 1] > coors[:, 3]))] = 0\n",
    "\n",
    "# Discard bboxes with negative areas.\n",
    "areas = np.sqrt(np.multiply.reduce(coors[:, (2, 3)] - coors[:, (0, 1)], axis=-1))\n",
    "scale_mask = np.logical_and((areas > 0), (areas < np.inf))\n",
    "\n",
    "# Discard bboxes with scores less than 0.3\n",
    "argmax = np.argmax(probs, axis=-1)\n",
    "scores = confs*np.max(probs, axis=-1)\n",
    "score_mask = scores > 0.3\n",
    "\n",
    "pick_mask = np.logical_and(scale_mask, score_mask)\n",
    "coors, scores, argmax = coors[pick_mask], scores[pick_mask], argmax[pick_mask]\n",
    "bboxes = np.concatenate([coors, scores[:, None], argmax[:, None]], axis=-1)\n",
    "\n",
    "# Perform non_maximum_suppression\n",
    "# bboxes = np.array(bboxes)\n",
    "clss_in_img = list(set(bboxes[:, 5]))\n",
    "best_bboxes = []\n",
    "for cls in clss_in_img:\n",
    "    bboxes_cls = bboxes[bboxes[:, 5] == cls]\n",
    "    # Process 1: Determine whether the number of bounding boxes is greater than 0 \n",
    "    while len(bboxes_cls) > 0:\n",
    "        # Process 2: Select the bounding box with the highest score according to socre order A\n",
    "        argmax = np.argmax(bboxes_cls[:, 4])\n",
    "        best_bbox = bboxes_cls[argmax]\n",
    "        best_bboxes.append(best_bbox)\n",
    "\n",
    "        bboxes_cls = np.delete(bboxes_cls, argmax, axis=0)\n",
    "\n",
    "        # Process 3: Calculate this bounding box A and remain all iou of the bounding box and remove\n",
    "        # those bounding boxes whose iou value is higher than the thrsd.\n",
    "        ious = np.array([compute_giou(best_bbox[:4], bbox_cls[:4]) for bbox_cls in bboxes_cls])\n",
    "\n",
    "        bboxes_cls = bboxes_cls*(ious <= 0.45)[:, None]\n",
    "        bboxes_cls = bboxes_cls[bboxes_cls[:, 4] > 0]\n",
    "\n",
    "bboxes = best_bboxes\n",
    "# Draw bboxes\n",
    "img_h, img_w, _ = ori_img.shape\n",
    "\n",
    "hsv_tuples = [(idx/n_clss, 1, 1) for idx in idx2cls.keys()]\n",
    "colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "colors = list(map(lambda x: (int(x[0]*255.), int(x[1]*255.), int(x[2]*255.)), colors))\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(colors)\n",
    "random.seed(None)\n",
    "\n",
    "for bbox in bboxes:\n",
    "    coor = np.array(bbox[:4], dtype=np.int32)\n",
    "    score = bbox[4]\n",
    "    cls_idx = int(bbox[5])\n",
    "    bbox_color = colors[cls_idx]\n",
    "    bbox_thk = int(0.6*(img_h + img_w)/1000)\n",
    "    bbox_thk = 1 if bbox_thk < 1 else bbox_thk\n",
    "    font_scale = 0.75*bbox_thk\n",
    "    (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
    "\n",
    "    cv2.rectangle(img=ori_img, pt1=(x1, y1), pt2=(x2, y2), color=bbox_color, thickness=bbox_thk*2)\n",
    "\n",
    "    score_str = f\"{score:.1%}\" \n",
    "    label = f\"{idx2cls[cls_idx]} \" + score_str\n",
    "\n",
    "    (text_w, text_h), baseline = cv2.getTextSize(text=label, fontFace=cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                                                        fontScale=font_scale, thickness=bbox_thk)\n",
    "    cv2.rectangle(img=ori_img, pt1=(x1, y1), pt2=(x1+text_w, y1+text_h+baseline),\n",
    "                color=bbox_color, thickness=cv2.FILLED)\n",
    "    cv2.putText(img=ori_img, text=label, org=(x1, y1+12), fontFace=cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                fontScale=font_scale, color=(0, 0, 0), thickness=bbox_thk, lineType=cv2.LINE_AA)\n",
    "\n",
    "cv2_imshow(ori_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12036,
     "status": "aborted",
     "timestamp": 1618726133238,
     "user": {
      "displayName": "Jongbeom Kim",
      "photoUrl": "",
      "userId": "17252605958116038360"
     },
     "user_tz": -540
    },
    "id": "dTRgAsURHhDf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "YOLOv3(Training)_COCO dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
