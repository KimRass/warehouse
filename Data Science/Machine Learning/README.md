# Dataset & Data Set
- Source: https://english.stackexchange.com/questions/2120/which-is-correct-dataset-or-data-set
- Dataset for certain datasets
- Data set for any set for data in general.

# Data Density (or Sparsity)
- Source: https://datascience.foundation/discussion/data-science/data-sparsity
- In a database, sparsity and density describe the number of cells in a table that are empty (sparsity) and that contain information (density), though sparse cells are not always technically emptyâ€”they often contain a â€œ0â€ digit.
## Sparse Matrix & Dense Matrix
- Source: https://en.wikipedia.org/wiki/Sparse_matrix
- ***A sparse matrix or sparse array is a matrix in which most of the elements are zero.*** *There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns.* ***By contrast, if most of the elements are non-zero, the matrix is considered dense. The number of zero-valued elements divided by the total number of elements is sometimes referred to as the sparsity of the matrix.***
### Compressed Sparse Row (CSR)
- *The compressed sparse row (CSR) or compressed row storage (CRS) or Yale format represents a matrix M by three (one-dimensional) arrays, that respectively contain nonzero values, the extents of rows, and column indices.*
```python
from scipy.sparse import csr_matrix

vals = [2, 4, 3, 4, 1, 1, 2]
rows = [0, 1, 2, 2, 3, 4, 4]
cols = [0, 2, 5, 6, 14, 0, 1]
sparse_mat = csr_matrix((vals,  (rows,  cols)))
dense_mat = sparse_mat.todense()
```

# Real Data & Test Data

# Embedding
- Source: https://analyticsindiamag.com/machine-learning-embedding/#:~:text=An%20embedding%20is%20a%20low,of%20a%20high%2Ddimensional%20vector.&text=Embedding%20is%20the%20process%20of,the%20two%20are%20semantically%20similar.
- *Embedding is the process of converting high-dimensional data to low-dimensional data in the form of a vector in such a way that the two are semantically similar.*
- Embeddings of neural networks are advantageous because they can lower the dimensionality of categorical variables and represent them meaningfully in the altered space.
- Source: https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
- An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models.

# Datasets
## `mnist`
```python
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
```
## `reuters`
```python
(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)
```
## `cifar10`
```python
(x_tr, y_tr), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
```

# Feature Scaling
- Source: https://en.wikipedia.org/wiki/Feature_scaling
- Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.
- ***Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.***
- ***Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.***
- It's also important to apply feature scaling if regularization is used as part of the loss function (so that coefficients are penalized appropriately).
```python
sc.fit()
sc.transform()
sc.fit_transform()
```
## Min-Max Scaling
- Min-max scaling is the simplest method and consists in rescaling the range of features to scale the range in [0, 1].
```python
x_new = (x - min(X))/(max(X) - min(X))
```
```python
from sklearn.preprocessing import MinMaxScaler

sc = MinMaxScaler()
```
## Standard Scaling
```python
import numpy as np

x_new = (x - np.mean(X))/np.std(X)
```
```python
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
```
## Robust Scaler
```python
from sklearn.preprocessing import RobustScaler

sc = RobustScaler()
```
## Normalizer
```python
from sklearn.preprocessing import Normalizer

sc = Normalizer()
```

# PCA (Principle Component Analysis)
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
user_embs_pca = pca.fit_transform(user_embs)
user_embs_pca = pd.DataFrame(user_embs_pca, index=user_embs.index, columns=["x", "y"])
```

# Parameter
## Hyperparameter
- Source: https://en.wikipedia.org/wiki/Artificial_neural_network
- A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning.

# Learning
- Source: https://en.wikipedia.org/wiki/Artificial_neural_network
- Learning is the adaptation of the network to better handle a task by considering sample observations. *Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0.* If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. *As long as its output continues to decline, learning continues.*
## Learning Rate
- *The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.*

# Discriminative & Generative Model
- Source: https://analyticsindiamag.com/what-are-discriminative-generative-models-how-do-they-differ/
- *Discriminative models draw boundaries in the data space, while generative ones model how data is placed throughout the space. Mathematically speaking, a discriminative machine learning trains a model by learning parameters that maximize the conditional probability P(Y|X), but a generative model learns parameters by maximizing the joint probability P(X,Y).*
## Discriminative Model
- ***The discriminative model is used particularly for supervised machine learning. Also called a conditional model, it learns the boundaries between classes or labels in a dataset. It creates new instances using probability estimates and maximum likelihood. However, they are not capable of generating new data points. The ultimate goal of discriminative models is to separate one class from another.***
## Generative Model
- ***Generative models are a class of statistical models that generate new data instances. These models are used in unsupervised machine learning to perform tasks such as probability and likelihood estimation, modelling data points, and distinguishing between classes using these probabilities. Generative models rely on the Bayes theorem to find the joint probability.***

# Convergence
- Source: https://en.wikipedia.org/wiki/Artificial_neural_network
- ***Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.***

# Batch Normalization
- Source: https://en.wikipedia.org/wiki/Batch_normalization 
- Batch normalizationÂ (also known asÂ batch norm) is a method used to makeÂ artificial neural networksÂ faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.[1]
While the effect of batch normalization is evident, the reasons behind its effectiveness remain under discussion. It was believed that it can mitigate the problem ofÂ internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network.[1]Â Recently, some scholars have argued that batch normalization does not reduce internal covariate shift, but rather smooths theÂ objective function, which in turn improves the performance.[2]Â However, at initialization, batch normalization in fact induces severeÂ gradient explosionÂ in deep networks, which is only alleviated by skip connections in residual networks.[3]Â Others sustain that batch normalization achieves length-direction decoupling, and thereby acceleratesÂ neural networks.[4]Â More recently a normalize gradient clipping technique and smart hyperparameter tuning has been introduced in Normalizer-Free Nets, so called "NF-Nets" which mitigates the need for batch normalization.[5][6

# MLOps
- Source: https://en.wikipedia.org/wiki/MLOps
- MLOps or ML Ops is a set of practices that aims to deploy and maintain machine learning models in production reliably and efficiently.[1] The word is a compound of "machine learning" and the continuous development practice of DevOps in the software field. Machine learning models are tested and developed in isolated experimental systems. When an algorithm is ready to be launched, MLOps is practiced between Data Scientists, DevOps, and Machine Learning engineers to transition the algorithm to production systems.

# Google Colab
## Mount Google Drive
```python
from google.colab import drive
import os
import sys

drive.mount("/content/drive", force_remount=True)
try:
    my_path = "/content/notebooks"
    os.symlink("/content/drive/MyDrive/ColabNotebooks/my_env", my_path)
    sys.path.insert(0, my_path)
except:
    print("Failed!")
os.chdir(my_path)
```
## Display Hangul
```python
import matplotlib as mpl
import matplotlib.pyplot as plt

%config InlineBackend.figure_format = "retina"
!apt -qq -y install fonts-nanum
fpath = "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf"
fpath = "/NanumBarunGothic.ttf"
font = mpl.font_manager.FontProperties(fname=fpath, size=9)
plt.rc("font", family="NanumBarunGothic") 
mpl.font_manager._rebuild()
mpl.rcParams["axes.unicode_minus"] = False
```
## Prevent from Disconnecting.
```
function ClickConnect(){
    console.log("ì½”ë© ì—°ê²° ëŠê¹€ ë°©ì§€");
	document.querySelector("colab-toolbar-button#connect").click()}
setInterval(ClickConnect, 60*1000)
```
## Install Libraries Permanently
```python
!pip install --target=$my_path LIBRARY_NAME
```
## Use TPU
```python
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu="grpc://" + os.environ["COLAB_TPU_ADDR"])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
```
```python
strategy = tf.distribute.experimental.TPUStrategy(resolver)
with strategy.scope():
    model = create_model()
    hist = model.fit()
```
## Install `khaiii`
```python
!git clone https://github.com/kakao/khaiii.git
!pip install cmake
!mkdir build
!cd build && cmake /content/khaiii
!cd /content/build/ && make all
!cd /content/build/ && make resource
!cd /content/build && make install
!cd /content/build && make package_python
!pip install /content/build/package_python
```

# Activation Function
- Source: https://leedakyeong.tistory.com/entry/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-%ED%99%9C%EC%84%B1%ED%99%94%ED%95%A8%EC%88%98%EB%9E%80-What-is-activation-function?category=845638
- í™œì„±í™”í•¨ìˆ˜ëŠ” ê¼­ ë¹„ì„ í˜• í•¨ìˆ˜ì´ì–´ì•¼ í•œë‹¤. ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì‹ ê²½ë§ì˜ ì¸µì„ ê¹Šê²Œ ìŒ“ëŠ” ê²ƒì— ì˜ë¯¸ê°€ ì—†ì–´ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ ì´ìœ ëŠ” ì˜ˆë¥¼ ë“¤ì–´, í™œì„±í™” í•¨ìˆ˜ë¥¼ h(x) = cx ë¼ëŠ” ì„ í˜•í•¨ìˆ˜ë¼ í•´ë³´ì. ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ë„¤íŠ¸ì›Œí¬ë¼ í•  ë•Œ, y(x) = h(h((x))) = c\*c\*c\*x = c^3\*xì´ë‹¤. ì´ëŠ” ê³§ y = axì—ì„œ a=c^3ê³¼ ê°™ë‹¤. ì¦‰, ê¸°ê» 3ì¸µì´ë‚˜ ìŒ“ì•˜ì§€ë§Œ 1ì¸µë§Œ ìŒ“ì€ ë„¤íŠ¸ì›Œí¬ì™€ ê°™ì•„ì§„ë‹¤. ì´ê²ƒì´ ë°”ë¡œ í™œì„±í•¨ìˆ˜ì˜ ì—­í• ì´ë‹¤.
- Source: 
http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221017173808
- 2ê°œë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œì¼ ë•ŒëŠ” Vanishing Gradient Problemë•Œë¬¸ì— sigmoidëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•Šê³ Â ReLUì™€ ê·¸ ë³€í˜•ëœ í™œì„±í™”í•¨ìˆ˜ë¥¼ ì£¼ë¡œ ì´ìš©í•œë‹¤. 3ê°œ ì´ìƒì„ ë¶„ë¥˜í•  ë•Œ ì£¼ë¡œ Softmaxì™€ ê·¸ ë³€í˜•ëœ í™œì„±í™”í•¨ìˆ˜ë¥¼ ì£¼ë¡œ ì´ìš©í•œë‹¤.
## Sigmoid Function
```python
def sigmoid(x):
    return 1/(1 + np.exp(-x))
```
```python
tf.math.sigmoid()
```
```python
tf.keras.activations.sigmoid(logits)
```
### Derivative of Sigmoid Function
```python
def deriv_sigmoid(x):
	return sigmoid(x)(1 - sigmoid(x))
```
## Softmax
- Source: http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221021710286&parentCategoryNo=&categoryNo=&viewDate=&isShowPopularPosts=false&from=postView
## Categorical Cross-Entropy Loss
- Source : https://wordbe.tistory.com/entry/ML-Cross-entropyCategorical-Binaryì˜-ì´í•´
- Softmax activation ë’¤ì— Cross-Entropy lossë¥¼ ë¶™ì¸ í˜•íƒœë¡œ ì£¼ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— Softmax loss ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤. â†’ Multi-class classificationì— ì‚¬ìš©ë©ë‹ˆë‹¤.
ìš°ë¦¬ê°€ ë¶„ë¥˜ë¬¸ì œì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” í™œì„±í™”í•¨ìˆ˜ì™€ ë¡œìŠ¤ì…ë‹ˆë‹¤. ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” MSE(mean square error) loss ë³´ë‹¤ CE lossê°€ ë” ë¹¨ë¦¬ ìˆ˜ë ´í•œ ë‹¤ëŠ” ì‚¬ì‹¤ì´ ì•Œë ¤ì ¸ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ multi classì—ì„œ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•  ë•Œ softmaxì™€ CE lossì˜ ì¡°í•©ì„ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.
## `tf.keras.activations.linear()`
## `tf.keras.activations.relu()`(= `"relu"`)

# Back Propogation
- Source: https://sacko.tistory.com/19
- ì§€ë‚œ ì‹œê°„ê¹Œì§€ëŠ” Inputì—ì„œ Outputìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ë©´ì„œ í™œì„±í™” í•¨ìˆ˜ë¥¼ í†µí•´ì„œ ê²°ê³¼ê°’ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒê¹Œì§€ ë°°ì› ë‹¤. ì´ë ‡ê²Œ ì­‰ ì˜¤ëŠ” ê²ƒì„ ìˆœì „íŒŒ(foward)ë¼ê³  í•˜ë©° ë§ ê·¸ëŒ€ë¡œ ì•ìª½ìœ¼ë¡œ input ê°’ì„ ì „íŒŒ, ë³´ë‚´ëŠ” ê²ƒì´ë¼ê³  ë³´ë©´ ëœë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ê°€ ì„ì˜ë¡œ í•œ ë²ˆ ìˆœì „íŒŒ í–ˆë‹¤ê³  ì¶œë ¥ ê°’ì´ ì •í™•í•˜ì§€ëŠ” ì•Šì„ ê²ƒì´ë‹¤. ìš°ë¦¬ê°€ ì„ì˜ë¡œ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ ê°’ì´ inputì— ì˜í•´ì„œ í•œ ë²ˆ ì—…ë°ì´íŠ¸ ë˜ê¸´ í–ˆì§€ë§Œ ë§ì€ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆë‹¤.
ì—­ì „íŒŒ ë°©ë²•ì€ ê²°ê³¼ ê°’ì„ í†µí•´ì„œ ë‹¤ì‹œ ì—­ìœ¼ë¡œ input ë°©í–¥ìœ¼ë¡œ ì˜¤ì°¨ë¥¼ ë‹¤ì‹œ ë³´ë‚´ë©° ê°€ì¤‘ì¹˜ë¥¼ ì¬ì—…ë°ì´íŠ¸ í•˜ëŠ” ê²ƒì´ë‹¤. ë¬¼ë¡  ê²°ê³¼ì— ì˜í–¥ì„ ë§ì´ ë¯¸ì¹œ ë…¸ë“œ(ë‰´ëŸ°)ì— ë” ë§ì€ ì˜¤ì°¨ë¥¼ ëŒë ¤ì¤„ ê²ƒì´ë‹¤.
- ê²°ê³¼ê°’ì€ ì˜¤ì°¨(error)ë¥¼ ê°€ì§€ê²Œ ë˜ëŠ”ë° ì—­ì „íŒŒëŠ” ì´ ì˜¤ì°¨(error)ë¥¼ ë‹¤ì‹œ ì—­ë°©í–¥ìœ¼ë¡œ hidden layerì™€ input layerë¡œ ì˜¤ì°¨ë¥¼ ë‹¤ì‹œ ë³´ë‚´ë©´ì„œ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ë©´ì„œ outputì—ì„œ ë°œìƒí–ˆë˜ ì˜¤ì°¨ë¥¼ ì ìš©ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.
í•œ ë²ˆ ëŒë¦¬ëŠ” ê²ƒì„ 1 epoch ì£¼ê¸°ë¼ê³  í•˜ë©° epochë¥¼ ëŠ˜ë¦´ ìˆ˜ë¡ ê°€ì¤‘ì¹˜ê°€ ê³„ì† ì—…ë°ì´íŠ¸(í•™ìŠµ)ë˜ë©´ì„œ ì ì  ì˜¤ì°¨ê°€ ì¤„ì–´ë‚˜ê°€ëŠ” ë°©ë²•ì´ë‹¤. 
- ìœ„ì˜ ê·¸ë¦¼ì„ ë³´ë©´ Output layerì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ ê°’ì´ ê°€ì§„ ì˜¤ì°¨ê°€ 0.6ì´ë¼ê³  ë˜ì–´ ìˆë‹¤. ì´ì „ ë…¸ë“œ(ë‰´ëŸ°ì—ì„œ) Output layerì— ê°ê° 3, 2ë¼ëŠ” ê°’ì„ ì „ë‹¬í•˜ì˜€ê¸° ë•Œë¬¸ì— Ouputì˜ Errorì— ìœ„ ë…¸ë“œëŠ” 60%, ì•„ë˜ ë…¸ë“œëŠ” 40% ì˜í–¥ì„ ì£¼ì—ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê· ë“±í•˜ê²Œ ê°€ì¤‘ì¹˜ë¥¼ ë‚˜ëˆ ì¤„ ìˆ˜ ìˆì§€ë§Œ ì˜í–¥ì„ ë¯¸ì¹œ ë§Œí¼ ì˜¤ì°¨ë¥¼ ë‹¤ì‹œ ì—­ì „íŒŒí•˜ëŠ”ê²Œ ë§ëŠ” ê²ƒ ê°™ë‹¤.
error 0.6ì„ 0.6, 0.4ë¥¼ ê³±í•˜ë‹ˆ  ìœ„ ë…¸ë“œì—ëŠ” errorê°€ 0.36ì´, ì•„ë˜ ë…¸ë“œì—ëŠ” 0.24ê°€ ì „ë‹¬ëœë‹¤. 
ì˜¤ì°¨ ì—­ì „íŒŒëŠ” ë§ ê·¸ëŒ€ë¡œ ì´ë ‡ê²Œ ì˜¤ì°¨ë¥¼ ì ì  ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©´ì„œ ë‹¤ì‹œ ì „íŒŒí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
- ì•ì—ì„œ ì˜¤ì°¨ê°€ ì—­ì „íŒŒë˜ëŠ” ê²ƒì„ ì•Œì•„ë³´ì•˜ëŠ”ë° ì˜¤ì°¨ë¥¼ ì—­ì „íŒŒí•˜ì—¬ ê³„ì† ì—…ë°ì´íŠ¸ í•˜ëŠ” ì´ìœ ëŠ” ì‹ ê²½ë§ì„ í†µí•´ ë” ë‚˜ì€ ê²°ê³¼ ê°’ì„ ë‚´ê¸° ìœ„í•´ì„œ weightë¥¼ ì¡°ì •í•˜ëŠ”ë° ì˜¤ì°¨ê°€ ì˜í–¥ì„ ì£¼ê¸° ë•Œë¬¸ì´ë‹¤. ìœ„ì˜ ì˜ˆì²˜ëŸ¼ ê°„ë‹¨í•œ ì‹ ê²½ë§ì´ë¼ë©´ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•˜ëŠ” ì‹ì€ êµ‰ì¥íˆ ê°„ë‹¨í•  ê²ƒì´ì§€ë§Œ íš¨ê³¼ì ì¸ ì‹ ê²½ë§ì€ ì ˆëŒ€ ì €ë ‡ê²Œ ê°„ë‹¨í•˜ì§€ ì•Šë‹¤. ìˆ˜ì‹­, ìˆ˜ë°±ê°œì˜ ë…¸ë“œ(ë‰´ëŸ°)ì´ ì—°ê²°ë˜ì–´ì„œ ìˆ˜ë§ì€ ê°€ì¤‘ì¹˜ì˜ ì¡°í•©ìœ¼ë¡œ íŠ¹ì • ë…¸ë“œì˜ weightë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤ë©´... íš¨ìœ¨ì ì¸ ë°©ë²•ì´ í•„ìš”í•  ê²ƒì´ë‹¤.
ê²½ì‚¬í•˜ê°•ë²•ì€ ë„ˆë¬´ë‚˜ ë§ì€ ì‹ ê²½ë§ ì•ˆì˜ ê°€ì¤‘ì¹˜ ì¡°í•©ì„ ëª¨ë‘ ê³„ì‚°í•˜ë©´ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¬ê¸° ë•Œë¬¸ì— íš¨ìœ¨ì ìœ¼ë¡œ ì´ë¥¼ í•˜ê¸°ìœ„í•´ ê³ ì•ˆëœ ë°©ë²•ì…ì´ë‹¤.
- Source: https://sacko.tistory.com/39?category=632408
- ì§€ë‚œ ì˜¤ì°¨ì—­ì „íŒŒ ê´€ë ¨ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì˜¤ì°¨ì—­ì „íŒŒë²•ì´ ìˆœì „íŒŒ(foward propagation)ë¡œ ê°€ì¤‘ì¹˜ í•™ìŠµì´ ë˜ê³  ì´ë¥¼ ê°±ì‹ í•˜ê¸° ìœ„í•´ì„œ ì˜¤ì°¨ë¥¼ ë°˜ì˜í•˜ì—¬ ë°˜ëŒ€ ë°©í–¥ì—ì„œ ë‹¤ì‹œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤ëŠ” ì‹ìœ¼ë¡œë§Œ ì„¤ëª…ì„ í–ˆë‹¤. ì—­ì „íŒŒë¥¼ ì‚¬ìš©í•˜ëŠ” ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ ì´ìœ ëŠ” ì—­ì „íŒŒë¥¼ í†µí•´ì„œ 'ë¯¸ë¶„'ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

# Gradient Descent
- Source: https://en.wikipedia.org/wiki/Gradient_descent
- ***Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent.*** Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.
## Optimizers
### Stochastic Gradient Descent (SGD)
```python
from tensorflow.keras.optimizers import SGD
```
### Adam
```python
from tensorflow.keras.optimizers import Adam
```
### Adagrad
```python
from tensorflow.keras.optimizers import Adagrad
```
- Source: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad
- *Adagrad tends to benefit from higher initial learning rate values compared to other optimizers.*

# Categorical Variables
- Sources: https://homeproject.tistory.com/4, http://blog.naver.com/PostView.nhn?blogId=choco_9966&logNo=221374544814&parentCategoryNo=&categoryNo=77&viewDate=&isShowPopularPosts=false&from=postView, https://dailyheumsi.tistory.com/120, https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02
## One-hot Encoding
- One-Hot encoding should not be performed if the number of categories are high. This would result in a sparse data.
- Decision trees does not require doing one-hot encoding. Since xgboost, AFAIK, is a boosting of decision trees, I assume the encoding is not required.
- í”¼ì²˜ë‚´ ê°’ë“¤ì´ ì„œë¡œ ë¶„ë¦¬ ë˜ì–´ìˆê¸° ë•Œë¬¸ì—, ìš°ë¦¬ê°€ ëª¨ë¥¼ ìˆ˜ ìˆëŠ” ì–´ë–¤ ê´€ê³„ë‚˜ ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤.
- features ë‚´ ê°’ì˜ ì¢…ë¥˜ê°€ ë§ì„ ê²½ìš°(High Cardinaliry), ë§¤ìš° ë§ì€ Feature ë“¤ì„ ë§Œë“¤ì–´ ë‚¸ë‹¤. ì´ëŠ”, ëª¨ë¸ í›ˆë ¨ì˜ ì†ë„ë¥¼ ë‚®ì¶”ê³  í›ˆë ¨ì— ë” ë§ì€ ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ê²Œ í•œë‹¤.(ì°¨ì›ì˜ ì €ì£¼ ë¬¸ì œ)
- ë‹¨ìˆœíˆ 0ê³¼ 1ë¡œë§Œ ê²°ê³¼ë¥¼ ë‚´ì–´ í° ì •ë³´ì´ë“ ì—†ì´ Tree ì˜ depth ë§Œ ê¹Šê²Œ ë§Œë“ ë‹¤. ì¤‘ìš”í•œê±´, Tree Depth ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒì— ë¹„í•´, 2ê°€ì§€ ê²½ìš°ë¡œë§Œ íŠ¸ë¦¬ë¥¼ ë§Œë“¤ì–´ ë‚˜ê°„ë‹¤ëŠ” ê²ƒì´ë‹¤.
- Random Forest ì™€ ê°™ì´, ì¼ë¶€ Feature ë§Œ Sampling í•˜ì—¬ íŠ¸ë¦¬ë¥¼ ë§Œë“¤ì–´ë‚˜ê°€ëŠ” ê²½ìš°, One-hot Feature ë¡œ ìƒì„±ëœ Feature ì˜ ìˆ˜ê°€ ë§ê¸° ë•Œë¬¸ì— ì´ featuresê°€ ë‹¤ë¥¸ featuresë³´ë‹¤ ë” ë§ì´ ì“°ì¸ë‹¤.
```python
import tensorflow as tf

tf.keras.utils.to_categorical([2, 5, 1, 6, 3, 7])
```
## Label Encoding
- just mapping randomly to ints often works very well, especially if there arenâ€™t to many.
- Another way is to map them to their frequency in the dataset. If youâ€™re afraid of collisions, map to counts + a fixed (per category) small random perturbation.
- ê°’ì˜ í¬ê¸°ê°€ ì˜ë¯¸ê°€ ì—†ëŠ”ë° ë¶€ì—¬ë˜ì–´ì„œ biasê°€ ìƒê¸¸ ìˆ˜ ìˆìŒ.
- One major issue with this approach is there is no relation or order between these classes, but the algorithm might consider them as some order, or there is some relationship.
- pandas factorizeëŠ” NANê°’ì„ ìë™ìœ¼ë¡œ -1ë¡œ ì±„ìš°ëŠ”ë° ë°˜í•´ scikitlearn Label EncoderëŠ” ì—ëŸ¬ë¥¼ ë°œìƒ.
- Categorical ê°’ë“¤ì€ ìˆœì„œê°œë…ì´ ì—†ìŒì—ë„, ëª¨ë¸ì€ ìˆœì„œê°œë…ì„ ì „ì œí•˜ì—¬ í•™ìŠµí•˜ê²Œ ëœë‹¤.
- Using `pandas`
	```python
	data["var"] = pd.Categorical(data["var"])
	
	vars = data["var"]
	vars_enc = data["var"].cat.codes
	```
- Using `sklearn.preprocessing.LabelEncoder()`
	```python
	from sklearn.preprocessing import LabelEncoder
	
	enc = LabelEncoder()

	enc.fit()
	enc.transform()
	enc.fit_transform()
	enc.inverse_transform()
	enc.classes_
	```
## Ordinal Encoding
- We do Ordinal encoding to ensure the encoding of variables retains the ordinal nature of the variable. This is reasonable only for ordinal variables, as I mentioned at the beginning of this article. This encoding looks almost similar to Label Encoding but slightly different as Label coding would not consider whether variable is ordinal or not and it will assign sequence of integers
- If we consider in the temperature scale as the order, then the ordinal value should from cold to â€œVery Hot. â€œ Ordinal encoding will assign values as ( Cold(1) <Warm(2)<Hot(3)<â€Very Hot(4)). Usually, we Ordinal Encoding is done starting from 1.
## Mean Encoding
- ì¼ë°˜ì ì¸ ë°©ë²•ì¸ categoryë¥¼ label encodingìœ¼ë¡œ ì–»ì€ ìˆ«ìëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì˜¤í•´í•˜ê¸° ì‰½ë‹¤.
- target valueì˜ ë” í° ê°’ì— í•´ë‹¹í•˜ëŠ” ë²”ì£¼ê°€ ë” í° ìˆ«ìë¡œ ì¸ì½”ë”© ë˜ê²Œ í•˜ëŠ” ê²ƒì´ë‹¤.
- Target encoding can lead to data leakage. To avoid that, fit the encoder only on your train set, and then use its transform method to encode categories in both train and test sets.(trainì— encoding fitting í›„ ê·¸ê²ƒì„ testì™€ train ì ìš©)
- íŠ¹íˆ Gradient Boosting Tree ê³„ì—´ì— ë§ì´ ì“°ì´ê³  ìˆë‹¤.
- ì˜¤ë²„í”¼íŒ…ì˜ ë¬¸ì œê°€ ìˆë‹¤. í•˜ë‚˜ëŠ” Data Leakage ë¬¸ì œì¸ë°, ì‚¬ì‹¤ í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì˜ˆì¸¡ ê°’ì— ëŒ€í•œ ì •ë³´ê°€ ì „í˜€ ë“¤ì–´ê°€ë©´ ì•ˆë˜ëŠ”ê²Œ ì¼ë°˜ì ì´ë‹¤. ê·¸ëŸ°ë°, Mean encoding ê³¼ì •ì„ ë³´ë©´, ì‚¬ì‹¤ encoding ëœ ê°’ì—ëŠ” ì˜ˆì¸¡ ê°’ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ìˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œëŠ” ëª¨ë¸ì„ Training set ì—ë§Œ ì˜¤ë²„í”¼íŒ… ë˜ë„ë¡ ë§Œë“ ë‹¤.
- ë‹¤ë¥¸ í•˜ë‚˜ëŠ” í•˜ë‚˜ì˜ label ê°’ì˜ ëŒ€í‘œê°’ì„ trainsetì˜ í•˜ë‚˜ì˜ mean ìœ¼ë¡œë§Œ ì‚¬ìš©í•œë‹¤ëŠ” ì ì´ë‹¤. ë§Œì•½ testset ì— í•´ë‹¹ label ê°’ì˜ í†µê³„ì ì¸ ë¶„í¬ê°€ trainset ê³¼ ë‹¤ë¥´ë‹¤ë©´, ì˜¤ë²„í”¼íŒ…ì´ ì¼ì–´ë‚  ìˆ˜ ë°–ì— ì—†ë‹¤. íŠ¹íˆ, ì´ëŸ° ìƒí™©ì€ Categorical ë³€ìˆ˜ ë‚´ Labelì˜ ë¶„í¬ê°€ ë§¤ìš° ê·¹ë‹¨ì ì¸ ê²½ìš°ì— ë°œìƒí•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ Trainset ì—ëŠ” ë‚¨ìê°€ 100ëª…, ì—¬ìê°€ 5ëª…ì´ê³ , Testset ì—ëŠ” 50ëª…, 50ëª…ì´ë¼ê³  í•˜ì. ìš°ë¦¬ëŠ” Trainset ìœ¼ë¡œ Mean encoding í• í…ë°, ì—¬ì 5ëª…ì˜ í‰ê· ê°’ì´ Testset ì˜ ì—¬ì 50ëª…ì„ ëŒ€í‘œí•  ìˆ˜ ìˆì„ê¹Œ? ì–´ë ¤ìš¸ ìˆ˜ ë°–ì— ì—†ë‹¤.
- Usually, Mean encoding is notorious for over-fitting; thus, a regularization with cross-validation or some other approach is a must on most occasions.
## Frequency Encoding
- ê°’ ë¶„í¬ì— ëŒ€í•œ ì •ë³´ê°€ ì˜ ë³´ì¡´. ê°’ì˜ ë¹ˆë„ê°€ íƒ€ê²Ÿê³¼ ì—°ê´€ì´ ìˆìœ¼ë©´ ì•„ì£¼ ìœ ìš©.
-Encodingí•œ Categoryë¼ë¦¬ ê°™ì€ Frequencyë¥¼ ê°€ì§„ë‹¤ë©´ Featureë¡œ ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ ë¨.(?)
## Grouping
- youâ€™ll need to do some exploratory data analysis to do some feature engineering like grouping categories or tactfully assigning appropriate integer values to match the relation of the variable with the output.
- if you know something about the categories you can perhaps group them and add an additional feature group id then order them by group id.
# `category_encoders`
```python
!pip install --upgrade category_encoders
```
```python
import category_encoders as ce
```
## `ce.target_encoder`
### `ce.target_encoder.TargetEncoder()`
```python
encoder = ce.target_encoder.TargetEncoder(cols=["company1"])
encoder.fit(data["company1"], data["money"]);
data["company1_label"] = encoder.transform(data["company1"]).round(0)
```

# Splitting Dataset
- Source: https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data
## Random Sampling
- The benefit of this approach is that we can see how the model reacts to previously unseen data.
However, what if one subset of our data only have people of a certain age or income levels? This is typically referred to as a sampling bias:
Sampling bias is systematic error due to a non-random sample of a population, causing some members of the population to be less likely to be included than others, resulting in a biased sample.
- If only use a train/test split, then I would advise comparing the distributions of your train and test sets. If they differ significantly, then you might run into problems with generalization. Use Facets to easily compare their distributions.
- Using `sklearn.model_selection.train_test_split()`
	```python
	from sklearn.model_selection import train_test_split

	tr_X, te_X, tr_y, te_y = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=3)
	```
	- `stratif`: If not None, data is split in a stratified fashion, using this as the class labels.
	
# Cross Validation (CV)
## Holdout Set
- When optimizing the hyperparameters of your model, you might overfit your model if you were to optimize using the train/test split.
Why? Because the model searches for the hyperparameters that fit the specific train/test you made.
## K-Fold CV
- We typically choose either i=5 or k=10 as they find a nice balance between computational complexity and validation accuracy:
```python
from sklearn.model_selection import KFold
```
### Stratified K-Fold CV
```python
from sklearn.model_selection import StratifiedKFold
```
## Group K-Fold CV
```python
from sklearn.model_selection import GroupKFold
```
### Stratified Group K-Fold CV
```python
from sklearn.model_selection import StratifiedGroupKFold
```
## Leave-One-Out CV
- This variant is identical to k-fold CV when k = n (number of observations).
```python
from sklearn.model_selection import LeaveOneOut

loo = LeaveOneOut()
```
## Leave-One-Group-Out CV
```python
from sklearn.model_selection import LeaveOneGroupOut
```
## Nested CV
- When you are optimizing the hyperparameters of your model and you use the same k-Fold CV strategy to tune the model and evaluate performance you run the risk of overfitting. You do not want to estimate the accuracy of your model on the same split that you found the best hyperparameters for.
- Instead, we use a Nested Cross-Validation strategy allowing to separate the hyperparameter tuning step from the error estimation step.
- The inner loop for hyperparameter tuning and
the outer loop for estimating accuracy.
- You are free to select the cross-validation approaches you use in the inner and outer loops. For example, you can use Leave-one-group-out for both the inner and outer loops if you want to split by specific groups.

# Evaluation Metrics
## Regression Problem
### MSE(Mean Squared Error)
### RMSE(Root Mean Squared Error)
### MAE(Mean Absolute Error)
### MPE(Mean Percentage Error)
### MAPE(Mean Absolute Percentage Error)
### R-Squared
- Source: https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/
- Explained variance + Error variance = Total variance.
- However, this math works out correctly only for linear regression models. In nonlinear regression, these underlying assumptions are incorrect. Explained variance + Error variance DO NOT add up to the total variance! The result is that R-squared isnâ€™t necessarily between 0 and 100%.
- If you use R-squared for nonlinear models, their study indicates you will experience the following problems:
R-squared is consistently high for both excellent and appalling models.
R-squared will not rise for better models all of the time.
If you use R-squared to pick the best model, it leads to the proper model only 28-43% of the time.
### Adjusted R-Squared
### RMSLE(Root Mean SquaredÂ Logarithmic Error)
- Source: https://shryu8902.github.io/machine%20learning/error/
## Classification Problem
### Confusion Matrix
- Source: https://datascienceschool.net/view-notebook/731e0d2ef52c41c686ba53dcaf346f32/
- ì •ë‹µ í´ë˜ìŠ¤ì™€ ì˜ˆì¸¡ í´ë˜ìŠ¤ì˜ ì¼ì¹˜ ì—¬ë¶€ë¥¼ ì„¼ ê²°ê³¼. ì •ë‹µ í´ë˜ìŠ¤ëŠ” í–‰(row)ìœ¼ë¡œ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ëŠ” ì—´(column)ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.
### Binary Classification Problem
- A, B í´ë˜ìŠ¤ ì¤‘ B í´ë˜ìŠ¤ë¥¼ ë§íˆëŠ” ë¬¸ì œë¼ê³  ê°€ì •í–ˆì„ ë•Œ
### Accuracy(ì •í™•ë„)
- ì „ì²´ ìƒ˜í”Œ ì¤‘ A ë˜ëŠ” Bë¼ê³  ë§ê²Œ ì˜ˆì¸¡í•œ ìƒ˜í”Œ ìˆ˜ì˜ ë¹„ìœ¨
### Precision(ì •ë°€ë„)
- B í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ì¶œë ¥í•œ ìƒ˜í”Œ ì¤‘ ì‹¤ì œë¡œ B í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ìƒ˜í”Œ ìˆ˜ì˜ ë¹„ìœ¨
### Recall(ì¬í˜„ìœ¨)
- ì‹¤ì œ B í´ë˜ìŠ¤ì— ì†í•œ í‘œë³¸ ì¤‘ì— B í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ì¶œë ¥í•œ í‘œë³¸ì˜ ìˆ˜ì˜ ë¹„ìœ¨
### F1 Ccore(ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· )
### Fall-out(ìœ„ì–‘ì„±ë¥ )
- ì‹¤ì œ B í´ë˜ìŠ¤ì— ì†í•˜ì§€ ì•ŠëŠ” í‘œë³¸ ì¤‘ì— B í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ì¶œë ¥í•œ í‘œë³¸ì˜ ë¹„ìœ¨
- Source: https://datascienceschool.net/view-notebook/731e0d2ef52c41c686ba53dcaf346f32/
### ROC(Receiver Operator Characteristic) Curve
- Source: https://datascienceschool.net/view-notebook/731e0d2ef52c41c686ba53dcaf346f32/
- ìœ„ì—ì„œ ì„¤ëª…í•œ ê°ì¢… í‰ê°€ ì ìˆ˜ ì¤‘ ì¬í˜„ìœ¨(recall)ê³¼ ìœ„ì–‘ì„±ë¥ (fall-out)ì€ ì¼ë°˜ì ìœ¼ë¡œ ì–‘ì˜ ìƒê´€ ê´€ê³„ê°€ ìˆë‹¤.
ì¬í˜„ìœ¨ì„ ë†’ì´ê¸° ìœ„í•´ì„œëŠ” ì–‘ì„±ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ê¸°ì¤€(threshold)ì„ ë‚®ì¶”ì–´ ì•½ê°„ì˜ ì¦ê±°ë§Œ ìˆì–´ë„ ì–‘ì„±ìœ¼ë¡œ íŒë‹¨í•˜ë„ë¡ í•˜ë©´ ëœë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë ‡ê²Œ ë˜ë©´ ìŒì„±ì„ì—ë„ ì–‘ì„±ìœ¼ë¡œ íŒë‹¨ë˜ëŠ” í‘œë³¸ ë°ì´í„°ê°€ ê°™ì´ ì¦ê°€í•˜ê²Œ ë˜ì–´ ìœ„ì–‘ì„±ìœ¨ì´ ë™ì‹œì— ì¦ê°€í•œë‹¤. ë°˜ëŒ€ë¡œ ìœ„ì–‘ì„±ìœ¨ì„ ë‚®ì¶”ê¸° ìœ„í•´ ì–‘ì„±ì„ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ì„ ì—„ê²©í•˜ê²Œ ë‘ê²Œ ë˜ë©´ ì¦ê±° ë¶€ì¡±ìœ¼ë¡œ ìŒì„± íŒë‹¨ì„ ë°›ëŠ” í‘œë³¸ ë°ì´í„°ì˜ ìˆ˜ê°€ ê°™ì´ ì¦ê°€í•˜ë¯€ë¡œ ì¬í˜„ìœ¨ì´ ë–¨ì–´ì§„ë‹¤.
- í´ë˜ìŠ¤ íŒë³„ ê¸°ì¤€ê°’ì˜ ë³€í™”ì— ë”°ë¥¸ ìœ„ì–‘ì„±ë¥ (fall-out)ê³¼ ì¬í˜„ìœ¨(recall)ì˜ ë³€í™”ë¥¼ ì‹œê°í™”í•œ ê²ƒì´ë‹¤.

# Data Augmentation
## DAR
- Source: http://faculty.bscb.cornell.edu/~hooker/darpaper.pdf
- Our motivation for DAR arose from the problem of extrapolation when using machine learning methods for prediction. Methods such as trees and
neural networks are guaranteed to give predictions in a bounded interval. The variance of these predictions may, nonetheless, be severe in regions far from observed data. 
## DARE
- Source: http://faculty.bscb.cornell.edu/~hooker/DARE.pdf
- The requirement that a function return to a base model away from training data is not easy to implement as part of the learning procedure in a universal approximator. However, we can achieve this behavior by stochastically generating uniform data with response given by the base model and adding it to the training data. The resulting procedure, which we termed Data-Augmented Regression for Extrapolation (DARE) can supplement any regression method.
- In this paper we have considered the problem of making predictions at points of extrapolation. Few learning procedures are designed to produce stable results at points of extrapolation and we show that even constant extrapolators can exhibit high variance away from training data. In order to stabilize these predictions and recognize their semi-arbitrary nature, we propose that predictions should be shrunk toward a base model in proportion to the density of training points near them. This follows the heuristic argument that as new examples get further away from known examples, our model predictions becomes less informed about the response. In order to carry out this shrinkage, we propose a very simple procedure of generating new uniformly distributed data, giving it the response associated with the base model and augmenting the training set with this data. Unless strong prior knowledge is available that pertains to the whole space, we recommend that an appropriate base model should be constant. This idea has the advantage that it can be applied to any learning method. Viewed from a Bayesian perspective, our method amounts to placing a random field prior on prediction values, basing predictions on a null model unless empirical data â€“ in the form of nearby training data â€“ provides evidence to the contrary. We also show that when linear regression is employed, our method is a stochastic form of ridge regression. The extent to which DARE regularizes will depend on the flexibility of the learner that is employed and the concentration of the training examples in predictor space. These are the factors that also influence the extent of our concern about extrapolation. Regularization
often also has a positive effect on predictive accuracy. We have demonstrated on simulated and real examples that it is possible to simultaneously improve predictive accuracy on the data distribution and stability at points of extrapolation.
## SMOGN(Synthetic Minority Over-Sampling Technique for Regression with Gaussian Noise)
- Source: https://github.com/nickkunz/smogn/blob/master/examples/smogn_example_3_adv.ipynb
- Here we cover the focus of this example. We call the smoter function from this package (smogn.smoter) and satisfy all of the available arguments for manual operation: data, y, samp_method, drop_na_col, drop_na_row, replace, k, rel_thres, rel_method, rel_ctrl_pts_rg
- The data argument takes a Pandas DataFrame, which contains the training set split. In this example, we input the previously loaded housing training set with follow input: data = housing
- The y argument takes a string, which specifies a continuous reponse variable by header name. In this example, we input 'SalePrice' in the interest of predicting the sale price of homes in Ames, Iowa with the following input: y = 'SalePrice'
- The k argument takes a positive integer less than ğ‘›, where ğ‘› is the sample size. k specifies the number of neighbors to consider for interpolation used in over-sampling. In this example, we input 7 to consider 2 additional neighbors (default is 5) with the following input: k = 7
- The pert argument takes a real number between 0 and 1. It represents the amount of perturbation to apply to the introduction of Gaussian Noise. In this example, we input 0.04 to increase the noise generated by synthetic examples where applicable (default is 0.02). We utilize the following input: pert = 0.04
- The samp_method argument takes a string, either 'balance' or 'extreme'. If 'balance' is specified, less over/under-sampling is conducted. If 'extreme' is specified, more over/under-sampling is conducted. In this case, we input 'balance' (default is 'balance') with the following input: samp_method = 'balance'
- The drop_na_col and drop_na_row arguments take a boolean. They specify whether or not to automatically remove features (columns) and observations (rows) that contain missing values (default is True for both). In this example, we make the argument explicit with the following inputs: drop_na_col = True and drop_na_row = True
- The replace argument takes a boolean. It specifies whether or not to utilize replacement in under-sampling (default is False). In this example, we make the argument explicit with the following input: replace = False
- The rel_thres argument takes a real number between 0 and 1. It specifies the threshold of rarity. The higher the threshold, the higher the over/under-sampling boundary. The inverse is also true, where the lower the threshold, the lower the over/under-sampling boundary. In this example, we dramatically reduce the boundary to 0.10 (default is 0.50) with the following input: rel_thres = 0.10
- The rel_method argument takes a string, either 'auto' or 'manual'. It specifies how relevant or rare "minority" values in y are determined. If 'auto' is specified, "minority" values are automatically determined by box plot extremes. If 'manual' is specified, "minority" values are determined by the user. In this example, we input 'manual' with the following input: rel_method = 'manual'
- The rel_ctrl_pts_rg argument takes a 2d array (matrix). It is used to manually specify the regions of interest or rare "minority" values in y. The first column indicates the y values of interest, the second column indicates a mapped value of relevance, either 0 or 1, where 0 is the least relevant and 1 is the most relevant, and the third column is indicative. It will be adjusted afterwards, use 0 in most cases.
The specified relevance values mapped to 1 are considered "minorty" values and are over-sampled. The specified relevance values mapped to 0 are considered "majority" values and are under-sampled.

# Decision Tree Algorithms
- Source: https://dailyheumsi.tistory.com/113?category=815369
## Feature Importance
- Source: https://stats.stackexchange.com/questions/162162/relative-variable-importance-for-boosting
- sum up the feature importances of the individual trees, then divide by the total number of trees.
- return the fmap, which has the counts of each time a  variable was split on
- How many times was this variable split on?
## Data Skewness
- Parametric methods are mainly based on the assumptions on the distribution of the data. They estimate a parameter (usually mean , sd ) from the sample data and is used in the modelling framework.
Point to ponder - Mean for a normal distribution will be different than mean for a right skewed distribution hence affecting how your model performs.
In Non Parametric methods no such feature of distribution is used for modelling. Primarily in Decision trees (say CART) it takes into account which variable/split brings in maximum difference in the two branches(eg - Gini) . In such a case , the distribution does not really matter.
- A positive aspect of using the error ratio instead of the error difference is that the feature importance measurements are comparable across different problems.
## Ensemble
- Source: https://lsjsj92.tistory.com/543?category=853217
## XGBoost
- Source: https://bcho.tistory.com/1354](https://bcho.tistory.com/1354, https://www.datacamp.com/community/tutorials/xgboost-in-python, https://xgboost.readthedocs.io/en/latest/parameter.html, https://brunch.co.kr/@snobberys/137, https://www.datacamp.com/community/tutorials/xgboost-in-python
### early_stopping
- Source: https://xgboost.readthedocs.io/en/latest/python/python_intro.html
- If thereâ€™s more than one metric in the eval_metric parameter given in params, the last metric will be used for early stopping.
### Customized Objective Function, Customized Metric Function
- Source: https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html
### Hyperparameters Tunning
- Source: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
#### `booster` [default=gbtree]
- Select the type of model to run at each iteration. It has 2 options:
gbtree: tree-based models
gblinear: linear models
#### `silent` [default=0]:
- Silent mode is activated is set to 1, i.e. no running messages will be printed. Itâ€™s generally good to keep it 0 as the messages might help in understanding the model.
#### `eta` [default=0.3] (= `learning_rate`)
- Analogous to learning rate in GBM
- Makes the model more robust by shrinking the weights on each step
- Typical final values to be used: 0.01-0.2
#### `min_child_weight` [default=1]
- Used to control over-fitting. **Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.**
- Too high values can lead to under-fitting hence, **it should be tuned using CV.**
#### `max_depth` [default=6]
- Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.
- ***Should be tuned using CV.***
#### `max_leaf_nodes`
- Can be defined in place of max\_depth. Since binary trees are created, a depth of â€˜nâ€™ would produce a maximum of 2^n leaves. If this is defined, GBM will ignore max\_depth.
#### `gamma` [default=0]
- A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.
- Makes the algorithm conservative. The values can vary depending on the loss function and **should be tuned**.
#### `max_delta_step` [default=0]
- In maximum delta step we allow each treeâ€™s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.
- Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.
This is generally not used but you can explore further if you wish.
#### `subsample` [default=1]
- Same as the subsample of GBM. Denotes the fraction of observations to be randomly sampled for each tree.
- **Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.**
- Typical values: 0.5-1
#### `colsample_bytree` [default=1]
- Similar to max_features in GBM. Denotes the fraction of columns to be randomly sampled for each tree.
Typical values: 0.5-1
#### `colsample_bylevel` [default=1]
- Denotes the subsample ratio of columns for each split, in each level. I donâ€™t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.
#### `lambda` [default=1] (=`reg_lambda`)
- L2 regularization term on weights (analogous to Ridge regression) .This is used to handle the regularization part of XGBoost. Though many data scientists donâ€™t use it often, **it should be explored to reduce overfitting.**
#### `alpha` [default=0]==reg_alpha
- L1 regularization term on weight (analogous to Lasso regression). **Can be used in case of very high dimensionality so that the algorithm runs faster when implemented.**
#### `scale_pos_weight` [default=1]
- **A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.**
#### `n_jobs`
- int Number of parallel threads used to run xgboost.
#### General Approach for Parameter Tuning
1. Choose a relatively **high learning rate**. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum **number of trees for this learning rat**e. XGBoost has a very useful function called as â€œcvâ€ which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.
2. Tune tree-specific parameters (**max\_depth, min\_child\_weight, gamma, subsample, colsample\_bytree**) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and Iâ€™ll take up an example here.
3. Tune **regularization parameters(lambda, alpha)** for xgboost which can help reduce model complexity and enhance performance.
4. **Lower the learning rate and decide the optimal parameters**.
Let us look at a more detailed step by step approach.
## Random Forest
- Source: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html?fbclid=IwAR1bPn1xldrpum8FWZEte0M7wPi9kE3BuFjSsyG9B4jSS5Th4oBkujEenNc
- the impurity-based feature importance of random forests suffers from being computed on statistics derived from the training dataset: the importances can be high even for features that are not predictive of the target variable, as long as the model has the capacity to use them to overfit.


# Feature Importance
## Permutation Feature Importance
- Source: https://scikit-learn.org/stable/modules/permutation_importance.html
- Permutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular.
- Features that are important on the training set but not on the held-out set might cause the model to overfit.
- Tree-based models provide an alternative measure of feature importances based on the mean decrease in impurity (MDI). Impurity is quantified by the splitting criterion of the decision trees (Gini, Entropy or Mean Squared Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting. Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data.
- Furthermore, impurity-based feature importance for trees are strongly biased and favor high cardinality features (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories.
- Permutation-based feature importances do not exhibit such a bias. Additionally, the permutation feature importance may be computed performance metric on the model predictions and can be used to analyze any model class (not just tree-based models).
- When two features are correlated and one of the features is permuted, the model will still have access to the feature through its correlated feature. This will result in a lower importance value for both features, where they might actually be important.
- One way to handle this is to cluster features that are correlated and only keep one feature from each cluster.
- Source: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py
- Because this dataset contains multicollinear features, the permutation importance will show that none of the features are important. One approach to handling multicollinearity is by performing hierarchical clustering on the featuresâ€™ Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster.
- The permutation importance plot shows that permuting a feature drops the accuracy by at most 0.012, which would suggest that none of the features are important.
- When features are collinear, permutating one feature will have little effect on the models performance because it can get the same information from a correlated feature. One way to handle multicollinear features is by performing hierarchical clustering on the Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster.
- Source: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py
- We will show that the impurity-based feature importance can inflate the importance of numerical features.
- Furthermore, the impurity-based feature importance of random forests suffers from being computed on statistics derived from the training dataset: the importances can be high even for features that are not predictive of the target variable, as long as the model has the capacity to use them to overfit.
- random_num is a high cardinality numerical variable (as many unique values as records).
random_cat is a low cardinality categorical variable (3 possible values).
The impurity-based feature importance ranks the numerical features to be the most important features. As a result, the non-predictive random_num variable is ranked the most important!
This problem stems from two limitations of impurity-based feature importances:
impurity-based importances are biased towards high cardinality features;
impurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity).
- It is also possible to compute the permutation importances on the training set. This reveals that random_num gets a significantly higher importance ranking than when computed on the test set. The difference between those two plots is a confirmation that the RF model has enough capacity to use that random numerical feature to overfit. You can further confirm this by re-running this example with constrained RF with min_samples_leaf=10.
- Source: https://christophm.github.io/interpretable-ml-book/feature-importance.html#feature-importance-data
- We measure the importance of a feature by calculating the increase in the model's prediction error after permuting the feature. A feature is "important" if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction. A feature is "unimportant" if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction.
- If you want a more accurate estimate, you can estimate the error of permuting feature j by pairing each instance with the value of feature j of each other instance (except with itself). This gives you a dataset of size n(n-1) to estimate the permutation error, and it takes a large amount of computation time. I can only recommend using the n(n-1) method if you are serious about getting extremely accurate estimates.
- **The feature importance based on training data makes us mistakenly believe that features are important for the predictions, when in reality the model was just overfitting and the features were not important at all.**
- Feature importance based on the training data tells us which features are important for the model in the sense that it depends on them for making predictions.
- If you would use (nested) cross-validation for the feature importance estimation, you would have the problem that the feature importance is not calculated on the final model with all the data, but on models with subsets of the data that might behave differently.
-  **You need to decide whether you want to know how much the model relies on each feature for making predictions (-> training data) or how much the feature contributes to the performance of the model on unseen data (-> test data).**
## Drop-out Feature Importance
- Source: https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e
## On Train Set or Test Set?
- Source: https://christophm.github.io/interpretable-ml-book/feature-importance.html
### On Test Set
- Really, it is one of the first things you learn in machine learning: If you measure the model error (or performance) on the same data on which the model was trained, the measurement is usually too optimistic, which means that the model seems to work much better than it does in reality. And since the permutation feature importance relies on measurements of the model error, we should use unseen test data. The feature importance based on training data makes us mistakenly believe that features are important for the predictions, when in reality the model was just overfitting and the features were not important at all.

# Feature Engineering
- Source: https://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/
## Time-Related Variables
- Time-stamp attributes are usually denoted by the EPOCH time or split up into multiple dimensions such as (Year, Month, Date, Hours, Minutes, Seconds). But in many applications, a lot of that information is unnecessary. Consider for example a supervised system that tries to predict traffic levels in a city as a function of Location+Time. In this case, trying to learn trends that vary by seconds would mostly be misleading. The year wouldnâ€™t add much value to the model as well. Hours, day and month are probably the only dimensions you need. So when representing the time, try to ensure that your model does require all the numbers you are providing it.
- Here is an example hypothesis: An applicant who takes days to fill in an application form is likely to be less interested / motivated in the product compared to some one who fills in the same application with in 30 minutes. Similarly, for a bank, time elapsed between dispatch of login details for Online portal and customer logging in might show customersâ€™ willingness to use Online portal. Another example is that a customer living closer to a bank branch is more likely to have a higher engagement than a customer living far off.
## Creating New Ratios and Proportions
- For example, in order to predict future performance of credit card sales of a branch, ratios like credit card sales / Sales person or Credit card Sales / Marketing spend would be more powerful than just using absolute number of card sold in the branch
## Creating Weights
- There may be domain knowledge that items with a weight above 4 incur a higher taxation rate. That magic domain number could be used to create a new binary feature Item_Above_4kg with a value of â€œ1â€ for our example of 6289 grams.
## Creating Aggregated Values
- You may also have a quantity stored as a rate or an aggregate quantity for an interval. For example, Num_Customer_Purchases aggregated over a year.
- For example, the following new binary features could be created: Purchases_Summer, Purchases_Fall, Purchases_Winter and Purchases_Spring.
## Splitting Features
- The Item_Weight could be split into two features: Item_Weight_Kilograms and Item_Weight_Remainder_Grams, with example values of 6 and 289 respectively.
## Bucketing, Binning, Discretization
- Sometimes, it makes more sense to represent a numerical attribute as a categorical one.
- Consider the problem of predicting whether a person owns a certain item of clothing or not. Age might definitely be a factor here. What is actually more pertinent, is the Age Group. So what you could do, is have ranges such as 1-10, 11-18, 19-25, 26-40, etc.
- It reduces overfitting in certain applications, where you donâ€™t want your model to try and distinguish between values that are too close by â€“ for example, you could club together all latitude values that fall in a city, if your property of interest is a function of the city as a whole.
- Binning also reduces the effect of tiny errors, by â€™rounding offâ€™ a given value to the nearest representative. Binning does not make sense if the number of your ranges is comparable to the total possible values, or if precision is very important to you.
- For example, you may have Item_Weight in grams, with a value like 6289. You could create a new feature with this quantity in kilograms as 6.289 or rounded kilograms like 6. If the domain is shipping data, perhaps kilograms is sufficient or more useful (less noisy) a precision for Item_Weight.
## Variables Transformation
- Transform complex non-linear relationships into linear relationships.Existence of a linear relationship between variables is easier to comprehend compared to a non-linear or curved relation. Transformation helps us to convert a non-linear relation into linear relation. Scatter plot can be used to find the relationship between two continuous variables. These transformations also improve the prediction. Log transformation is one of the commonly used transformation technique used in these situations
- For right skewed distribution, we take square / cube root or logarithm of variable and for left skewed, we take square / cube or exponential of variables.
- Cube root can be applied to negative values including zero. Square root can be applied to positive values including zero.
## Feature Crosses
- Feature crosses are a unique way to combine two or more categorical attributes into a single one. This is extremely useful a technique, when certain features together denote a property better than individually by themselves. Mathematically speaking, you are doing a cross product between all possible values of the categorical features.
Consider a feature A, with two possible values {A1, A2}. Let B be a feature with possibilities {B1, B2}. Then, a feature-cross between A & B (lets call it AB) would take one of the following values: {(A1, B1), (A1, B2), (A2, B1), (A2, B2)}. You can basically give these â€˜combinationsâ€™ any names you like. Just remember that every combination denotes a synergy between the information contained by the corresponding values of A and B.

# Distance Features
- Source: https://www.tandfonline.com/doi/full/10.1080/10095020.2018.1503775
- The Euclidean function is unrealistic for some (notably urban) settings which contain complex physical restrictions and social structures for example road and path networks, large restricted areas of private land and legal road restrictions such as speed limits and one-way systems.
## House prices in space
- Most contemporary analysis mimics this trend, for example predicting property value by using (1) the average sales price of other properties in the local comparable markets, (2) a spatial clustering of properties and demographics (Malczewski 2004) and (3) a local demographic â€œtrade areaâ€ (Daniel 1994).
- In the case of spatially dependent data, cross-validation is optimistic due to its inherent IID assumption.
- Euclidean distances are exclusively considered in all of the above work. This paper hypothesizes that house prices are related to a more complex structural network relating to (restricted) road distance and travel time; hence, we introduce an approximate (restricted) road distance and travel time metric using the Minkowski distance function for a valid house price Kriging predictor (Matheron 1963; Cressie 1990).
## Data Description
- Figure 1. A comparison of an Euclidean distance matrix versus a drive time distance matrix and a road distance matrix around the center point of Coventry. (a) Euclidean distance buffer from 0 to 4 miles around the centre of Coventry; (b) Travel time distance buffer from 0 to 10 minutes drive time around the centre of Coventry; (c) Road distance buffer from 0 to 4 miles around the centre of Coventry.
## Collapsing Time
- The price paid data for 2016 are addressed only (herewithin named ). This accounts for 3669 sales in Coventry. Stage 1 predicts each propertyâ€™s sale price based on its value on the 1 January 2017 (for time singularity). This process involves each property being assigned some percentage price change based on the date that it was sold and the lower super output area that the property is contained within to produce a value for all 3669 properties at the date 1 January 2017 (). The error for the purposes of this experiment is minimal or nonexistent due to the small temporal and spatial aggregate areas being considered.
- Figure 2 shows an exact example where the distance between houses  to  is 0.24 mi along the red dotted line which takes a route along â€œBrownshill Green Roadâ€ and is marked as a one-way system, this means that the route  to  must be different, which, in this case, is further; hence, the distance matrix is not symmetric. The same reasoning applies for a travel time matrix

# Time Feature
- Source https://medium.com/@andrejusb/machine-learning-date-feature-transformation-explained-4feb774c9dbe
## Splitting Features
- One of the ways is to split date value into multiple columns with numbers describing the original date (year, quarter, month, week, day of year, day of month, day of week).
## Date Feature Transformation into a Difference Between Dates
- We can use date difference as such:
â€” Day difference between Payment Due Date and Invoice Date
â€” Day difference between Payment Date and Invoice Date
This should bring clear pattern when there is payment delay â€” difference between payment date/invoice date will be bigger than between payment due date/invoice date.

# Explicit Data, Implicit Data
- source: https://orill.tistory.com/entry/Explicit-vs-Implicit-Feedback-Datasets
## Explicit Datasets
- ì˜í™” ì¶”ì²œì‹œìŠ¤í…œì„ ë§Œë“œëŠ” ìƒí™©ì„ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤. ì–´ë–¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?
- ê°€ì¥ ë¨¼ì € ë– ì˜¤ë¥´ëŠ” ë°ì´í„°ëŠ” ì‚¬ìš©ìì˜ í‰ì  ê¸°ë¡ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ ë¶„ë¥˜ì— MNISTê°€ ìˆë‹¤ë©´ ì¶”ì²œì‹œìŠ¤í…œì—ëŠ” Movielens Dataê°€ ìˆìŠµë‹ˆë‹¤. ratings.csv íŒŒì¼ì€ (userId,movieId,rating,timestamp) í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìœ ì €ê°€ ë³¸ ì˜í™”ì— ëŒ€í•´ì„œ 5.0ì ì„ ë§Œì ìœ¼ë¡œ 0.5ì  ë‹¨ìœ„ë¡œ í‰ê°€í•œ ë°ì´í„°ê°€ ì‹œê°„ê³¼ í•¨ê»˜ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤. WatchaëŠ” ì´ëŸ° ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì¶”ì²œ ì•±ì„ ì„œë¹„ìŠ¤í•˜ê³  ìˆê³  NeflixëŠ” í‰ì ëŒ€ì‹  ì¢‹ì•„ìš”, ì‹«ì–´ìš” ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤. í‰ì  ê¸°ë¡ì²˜ëŸ¼ ìœ ì €ê°€ ìì‹ ì˜ ì„ í˜¸ë„ë¥¼ ì§ì ‘(Explicit) í‘œí˜„í•œ Dataë¥¼ Explicit Dataë¼ê³  í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì¢…ë¥˜ì˜ Explicit Dataë¡œëŠ” ì˜í™” ë¦¬ë·°, êµ¬ë…, ì°¨ë‹¨ ë°ì´í„° ë“±ì´ ìˆìŠµë‹ˆë‹¤.
- Explicit Dataë¥¼ í†µí•´ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” ê°•ë ¥í•©ë‹ˆë‹¤. ìœ ì €ì˜ í˜¸ë¶ˆí˜¸ë¥¼ ëª…ë°±íˆ ì•Œ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ìœ ìš©ì„±ì´ ì¢‹ì€ ë°˜ë©´ ë°ì´í„°ë¥¼ ì–»ê¸° í˜ë“¤ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ì„ í•´ë³´ë©´ ì˜í™”ë¥¼ ë³´ê³  í‰ì ì„ ë§¤ê¸°ëŠ” ìœ ì €ì˜ ìˆ«ìëŠ” ì „ì²´ ìœ ì €ì— ë¹„í•´ ì ê³  ë¦¬ë·°ê¹Œì§€ ë‚¨ê¸°ëŠ” ë°ì´í„°ëŠ” í›¨ì”¬ ì ìŠµë‹ˆë‹¤. ìœ ì €ê°€ ì ê·¹ì ì¸ Actionì„ ì·¨í•´ì•¼ í•˜ëŠ”ë° ë‹¨ìˆœíˆ ê·€ì°®ê±°ë‚˜ ì‹¬ë¦¬ì ì¸ ê±°ë¶€ê°ì´ ìˆê¸° ë•Œë¬¸ì´ì£ . "ì¢‹ì•„ìš”ì™€ êµ¬ë… ë¶€íƒë“œë ¤ìš”"ë¼ëŠ” ë§ì€ ìœ ì €ë“¤ì´ ì›¬ë§Œí•´ì„œëŠ” 'ì¢‹ì•„ìš”'ë¥¼ ëˆŒëŸ¬ì£¼ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì„ ë°©ì¦í•˜ê³  ìˆëŠ” ë“¯í•©ë‹ˆë‹¤. 
## Implicit Datasets
- Collaborative Filtering For Implicit Feedback Datasets ë…¼ë¬¸ì— ë”°ë¥´ë©´ 2010ë…„ ì´ì „ê¹Œì§€ëŠ” ì¶”ì²œì‹œìŠ¤í…œ ë¶„ì•¼ì—ì„œ Explicit Dataë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì—°êµ¬ê°€ ì£¼ë¡œ ì´ë£¨ì–´ì¡Œë˜ ëª¨ì–‘ì…ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” Implicitë¼ëŠ” ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¶”ì²œì‹œìŠ¤í…œì„ ë§Œë“œëŠ” ë°©ë²•ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ë¶€í„°ëŠ” ì´ ë…¼ë¬¸ì—ì„œ ì†Œê°œí•œ Implicit Datasetsì˜ ê°œë…ê³¼ íŠ¹ì§•ì„ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤. Model ë¶€ë¶„ì— ì¶”í›„ì— í¬ìŠ¤íŒ…í•  ì˜ˆì •ì…ë‹ˆë‹¤.
- Implicit DataëŠ” ìœ ì €ê°€ ê°„ì ‘ì (Implicit)ìœ¼ë¡œ ì„ í˜¸, ì·¨í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆì‹œë¡œëŠ” ê²€ìƒ‰ ê¸°ë¡, ë°©ë¬¸ í˜ì´ì§€, êµ¬ë§¤ ë‚´ì—­ ì‹¬ì§€ì–´ ë§ˆìš°ìŠ¤ ì›€ì§ì„ ê¸°ë¡ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ë°ì´í„°ëŠ” ì¼ë‹¨ ìœ ì €ê°€ ê°œì¸ì •ë³´ì œê³µì— ë™ì˜ë§Œ í•œë‹¤ë©´ ìë™ì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ê¸° ë•Œë¬¸ì— ìˆ˜ì§‘ì˜ ë‚œì´ë„ ë‚®ê³  í™œìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ëŸ‰ì´ ë§ìŠµë‹ˆë‹¤.
- Implicit Datasetsì„ ë‹¤ë£° ë•Œ ì—¼ë‘í•´ë‘ì–´ì•¼ í•  ëª‡ ê°€ì§€ íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤.
	- ë¶€ì •ì ì¸ í”¼ë“œë°±ì´ ì—†ë‹¤(No Negative Feedback) : ìœ ì €ê°€ ì–´ë–¤ ì˜í™”ë¥¼ ë³¸ ê¸°ë¡ì´ ì—†ì„ ë•Œ ìœ ì €ê°€ ê·¸ ì˜í™”ë¥¼ ì‹«ì–´í•´ì„œ ë³´ì§€ ì•Šì•˜ëŠ”ì§€ í˜¹ì€ ê·¸ì € ì•Œì§€ ëª»í–ˆê¸° ë•Œë¬¸ì— ë³´ì§€ ì•Šì•˜ëŠ”ì§€ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ëŸ° ì°¨ì´ì  ë•Œë¬¸ì— Explicit Feedback Datasetì„ ë‹¤ë£° ë•ŒëŠ” ìˆ˜ì§‘ëœ ë°ì´í„°ì—ë§Œ ì§‘ì¤‘í•˜ê³  Unobserved DataëŠ” Missing Value ì·¨ê¸‰í•˜ì—¬ ëª¨ë¸ì„ ë§Œë“¤ì–´ë„ ê´œì°®ìŠµë‹ˆë‹¤. ìœ ì €ì˜ ë¶ˆí˜¸ ì •ë³´(ë‚®ì€ í‰ì , ì‹«ì–´ìš”)ê°€ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ Implicit Dataë¥¼ ëª¨ë¸ë§í•  ë•ŒëŠ” ìˆ˜ì§‘ë˜ì§€ ì•Šì€ ë°ì´í„°ë„ ê°™ì´ ëª¨ë¸ë§í•´ì•¼ í•©ë‹ˆë‹¤. ìˆ˜ì§‘ë˜ì§€ ì•Šì€ ë°ì´í„°ì— (í™•ì‹¤í•˜ì§€ëŠ” ì•Šì•„ë„) ë¶ˆí˜¸ ì •ë³´, ë¶€ì •ì ì¸ ì •ë³´ê°€ ë‹´ê²¨ ìˆì„ ê°€ëŠ¥ì„±ì´ í¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
	- ì• ì´ˆì— ì¡ìŒì´ ë§ë‹¤(Inherently Noisy) :  ë°˜ëŒ€ë¡œ ì–´ë–¤ ì˜í™”ë¥¼ ë´¤ë‹¤ê³  í•´ì„œ ìœ ì €ê°€ ê·¸ ì˜í™”ë¥¼ ì¢‹ì•„í•œë‹¤ê³  ë§í•˜ê¸° í˜ë“­ë‹ˆë‹¤. ê³¼ì œ ë•Œë¬¸ì— ì˜í™”ë¥¼ ë³¸ ê²ƒì¼ ìˆ˜ë„ ìˆê³  ì˜í™”ê°€ ë§ˆìŒì— ì•ˆë“¤ì§€ë§Œ êµ¬ë§¤í•œ ê²Œ ì•„ì‰¬ì›Œì„œ ëê¹Œì§€ ë³¸ ê²ƒì¼ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ìœ íŠœë¸Œì˜ ê²½ìš° ì‹œì²­ì‹œê°„ì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ë§ì´ ìˆëŠ”ë° ìœ ì €ê°€ ì˜ìƒì„ í‹€ì–´ë†“ê³  ì ë“¤ì—ˆì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
	- ìˆ˜ì¹˜ëŠ” ì‹ ë¢°ë„ë¥¼ ì˜ë¯¸í•œë‹¤.(The numerical value of implicit feedback indicates confidence) : Explicit Dataì˜ ê²½ìš° ë†’ì€ ìˆ˜ì¹˜ëŠ” ë†’ì€ ì„ í˜¸ë„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. 2ë²ˆì—ì„œ ë´¤ë“¯ì´ Implicit Dataì—ì„œëŠ” ë†’ì€ ìˆ˜ì¹˜ê°€ ê¼­ ë†’ì€ ì„ í˜¸ë„ë¥¼ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì¸ìƒ ì˜í™”ë¥¼ ë´¤ì–´ë„ ì‹œì²­ì‹œê°„ì€ 2ì‹œê°„ ì¦ˆìŒì¸ ë°˜ë©´ ê·¸ì € ê·¸ëŸ° ë“œë¼ë§ˆë¥¼ ë³´ëŠ” ê²½ìš°ì— 10ì‹œê°„ì„ ë„˜ê²Œ ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë†’ì€ ê°’ì€ ì‹ ë¢°í• ë§Œí•œ ë°ì´í„°ì„ì„ ì˜ë¯¸í•œë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œ ë²ˆ ë³´ê³ ë§Œ ì˜ìƒë³´ë‹¤ëŠ” ìì£¼, ì˜¤ë˜ ë³¸ ì˜ìƒì´ ìœ ì €ì˜ ì„ í˜¸ë„, ì˜ê²¬ì„ í‘œí˜„í–ˆì„ í™•ë¥ ì´ ë†’ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
	- Implicit-feedback Recommender Systemì˜ í‰ê°€ëŠ” ì ì ˆí•œ ë°©ë²•ì„ ê³ ë¯¼í•´ë´ì•¼ í•œë‹¤ : í‰ì  ë°ì´í„°ë¥¼ ì´ìš©í•˜ëŠ” ê²½ìš° ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì´ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ í‰ê°€í•˜ëŠ” Mean Squared Error ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ í¸ë¦¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹œì²­ì‹œê°„, í´ë¦­ìˆ˜, ì¡°íšŒ ê¸°ë¡ì„ ì´ìš©í•˜ëŠ” ê²½ìš° ì •ë‹µê°’ì„ ì£¼ê¸°ê°€ ì–´ë µìŠµë‹ˆë‹¤. ë”°ë¼ì„œ implicit modelì˜ ê²½ìš° itemì˜ availabilityë‚˜ ë°˜ë³µë˜ëŠ” feeback ë“±ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. availabilityë€ ë™ì‹œê°„ì— ë°©ì˜ë˜ëŠ” ë‘ TV Showì˜ ê²½ìš° í•œìª½ë§Œ ë³¼ ìˆ˜ ìˆì–´ì„œ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢‹ì•„í•œë‹¤ê³  í•´ë„ Implicit Dataê°€ ìŒ“ì´ì§€ ì•ŠëŠ” ìƒí™©ì„ ë§í•©ë‹ˆë‹¤. ë°˜ë³µë˜ëŠ” Feedbackì€ ìœ ì €ê°€ í•œ ë²ˆ ì´ìƒ í”„ë¡œê·¸ë¨ì„ ë´¤ì„ ë•Œ í•œ ë²ˆ ë³¸ ê²½ìš°ì™€ ì–´ë–»ê²Œ ë‹¤ë¥´ê²Œ í‰ê°€í•  ê²ƒì¸ê°€ì— ëŒ€í•œ ê³ ë ¤ì…ë‹ˆë‹¤.
## Collaborative Filtering For Implicit Feedback Datasets
- ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì•ì„œ ì–¸ê¸‰í•œ Implicit Datasetsì˜ íŠ¹ì§•ì„ ê³ ë ¤í•´ì„œ ì–´ë–»ê²Œ ì¶”ì²œ ëª¨ë¸ì„ ë§Œë“¤ê³  í‰ê°€í• ì§€ì— ëŒ€í•œ ë‚´ìš©ì„ ì •ë¦¬í•´ë³´ê² ìŠµë‹ˆë‹¤. ì§§ê²Œ ì •ë¦¬í•˜ìë©´ Unobserved Dataì™€ Observed Dataë¥¼ êµ¬ë¶„í•˜ê³  ë†’ì€ Confidence Dataì— ë†’ì€ Lossë¥¼ ì˜ë¯¸í•˜ëŠ” Loss Functionì„ ì •ì˜í•˜ì—¬ Matrix Factorizationì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. Evaluationì€ Recall ì§€í‘œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íƒ€ë‹¹í•˜ë‹¤ê³  ìƒê°í•˜ì—¬ Expected Percentile Rankingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

# Model Linearity
## Difference between Linear and Nonlinear
- Source: https://statisticsbyjim.com/regression/difference-between-linear-nonlinear-regression-models/
- The form is linear in the parameters because all terms are either the constant or a parameter multiplied by an independent variable (IV). A linear regression equation simply sums the terms. While the model must be linear in the parameters, you can raise an independent variable by an exponent to fit a curve. For instance, you can include a squared or cubed term.
Nonlinear regression models are anything that doesnâ€™t follow this one form.
While both types of models can fit curvature, nonlinear regression is much more flexible in the shapes of the curves that it can fit. After all, the sky is the limit when it comes to the possible forms of nonlinear models.
- While the independent variable is squared, the model is still linear in the parameters. Linear models can also contain log terms and inverse terms to follow different kinds of curves and yet continue to be linear in the parameters.
- If a regression equation doesnâ€™t follow the rules for a linear model, then it must be a nonlinear model
- Source: https://brunch.co.kr/@gimmesilver/18
- ë¹„ì„ í˜• ëª¨ë¸ì€ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë³€í˜•í•˜ë”ë¼ë„ íŒŒë¼ë¯¸í„°ë¥¼ ì„ í˜• ê²°í•©ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ì—†ëŠ” ëª¨ë¸ì„ ë§í•©ë‹ˆë‹¤. ì´ëŸ° ë¹„ì„ í˜• ëª¨ë¸ ì¤‘ ë‹¨ìˆœí•œ ì˜ˆë¡œëŠ” ì•„ë˜ì™€ ê°™ì€ ê²ƒì´ ìˆìŠµë‹ˆë‹¤. ì´ ì‹ì€ ì•„ë¬´ë¦¬ x, y ë³€ìˆ˜ë¥¼ ë³€í™˜í•˜ë”ë¼ë„ íŒŒë¼ë¯¸í„°ë¥¼ ì„ í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    ì„ í˜• íšŒê·€ ëª¨ë¸ì€ íŒŒë¼ë¯¸í„° ê³„ìˆ˜ì— ëŒ€í•œ í•´ì„ì´ ë‹¨ìˆœí•˜ì§€ë§Œ ë¹„ì„ í˜• ëª¨ë¸ì€ ëª¨ë¸ì˜ í˜•íƒœê°€ ë³µì¡í•  ê²½ìš° í•´ì„ì´ ë§¤ìš° ì–´ë µìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ë³´í†µ ëª¨ë¸ì˜ í•´ì„ì„ ì¤‘ì‹œí•˜ëŠ” í†µê³„ ëª¨ë¸ë§ì—ì„œëŠ” ë¹„ì„ í˜• íšŒê·€ ëª¨ë¸ì„  ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
    ê·¸ëŸ°ë° ë§Œì•½ íšŒê·€ ëª¨ë¸ì˜ ëª©ì ì´ í•´ì„ì´ ì•„ë‹ˆë¼ ì˜ˆì¸¡ì— ìˆë‹¤ë©´ ë¹„ì„ í˜• ëª¨ë¸ì€ ëŒ€ë‹¨íˆ ìœ ì—°í•˜ê¸° ë•Œë¬¸ì— ë³µì¡í•œ íŒ¨í„´ì„ ê°–ëŠ” ë°ì´í„°ì— ëŒ€í•´ì„œë„ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì¶©ë¶„íˆ ë§ì€ ë°ì´í„°ë¥¼ ê°–ê³  ìˆì–´ì„œ variance errorë¥¼ ì¶©ë¶„íˆ ì¤„ì¼ ìˆ˜ ìˆê³  ì˜ˆì¸¡ ìì²´ê°€ ëª©ì ì¸ ê²½ìš°ë¼ë©´ ë¹„ì„ í˜• ëª¨ë¸ì€ ì‚¬ìš©í• ë§Œí•œ ë„êµ¬ì…ë‹ˆë‹¤. ê¸°ê³„ í•™ìŠµ ë¶„ì•¼ì—ì„œëŠ” ì‹¤ì œ ì´ëŸ° ë¹„ì„ í˜• ëª¨ë¸ì„ ëŒ€ë‹¨íˆ ë§ì´ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë° ê°€ì¥ ëŒ€í‘œì ì¸ ê²ƒì´ ì†Œìœ„ ë”¥ ëŸ¬ë‹ì´ë¼ê³  ë¶€ë¥´ëŠ” ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.
- ì •ë¦¬í•˜ìë©´, ì„ í˜• íšŒê·€ ëª¨ë¸ì€ íŒŒë¼ë¯¸í„°ê°€ ì„ í˜•ì‹ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” íšŒê·€ ëª¨ë¸ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ëŸ° ì„ í˜• íšŒê·€ ëª¨ë¸ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì •í•˜ê±°ë‚˜ ëª¨ë¸ì„ í•´ì„í•˜ê¸°ê°€ ë¹„ì„ í˜• ëª¨ë¸ì— ë¹„í•´ ë¹„êµì  ì‰½ê¸° ë•Œë¬¸ì—, ë°ì´í„°ë¥¼ ì ì ˆíˆ ë³€í™˜í•˜ê±°ë‚˜ ë„ì›€ì´ ë˜ëŠ” featureë“¤ì„ ì¶”ê°€í•˜ì—¬ ì„ í˜• ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤ë©´ ì´ë ‡ê²Œ í•˜ëŠ” ê²ƒì´ ì ì€ ê°œìˆ˜ì˜ featureë¡œ ë³µì¡í•œ ë¹„ì„ í˜• ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒë³´ë‹¤ ì—¬ëŸ¬ ë©´ì—ì„œ ìœ ë¦¬í•©ë‹ˆë‹¤. ë°˜ë©´ ì„ í˜• ëª¨ë¸ì€ í‘œí˜„ ê°€ëŠ¥í•œ ëª¨ë¸ì˜ ê°€ì§“ìˆ˜(íŒŒë¼ë¯¸í„°ì˜ ê°œìˆ˜ê°€ ì•„ë‹ˆë¼ íŒŒë¼ë¯¸í„°ì˜ ê²°í•© í˜•íƒœ)ê°€ í•œì •ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ìœ ì—°ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ ë³µì¡í•œ íŒ¨í„´ì„ ê°–ê³  ìˆëŠ” ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì •í™•í•œ ëª¨ë¸ë§ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ìµœê·¼ì—ëŠ” ëª¨ë¸ì˜ í•´ì„ë³´ë‹¤ëŠ” ì •êµí•œ ì˜ˆì¸¡ì´ ì¤‘ìš”í•œ ë¶„ì•¼ì˜ ê²½ìš° ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì™€ ê°™ì€ ë¹„ì„ í˜• ëª¨ë¸ì´ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.

# Missing Value
## Treating Missing Values
- Source: https://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/
### Imputation(Mean, Mode, Median)
- Generalized Imputation: In this case, we calculate the mean or median for all non missing values of that variable then replace missing value with mean or median. Like in above table, variable â€œManpowerâ€ is missing so we take average of all non missing values of â€œManpowerâ€ (28.33) and then replace missing value with it.
Similar case Imputation: In this case, we calculate average for gender â€œMaleâ€ (29.75) and â€œFemaleâ€ (25) individually of non missing values then replace the missing value based on gender. For â€œMaleâ€œ, we will replace missing values of manpower with 29.75 and for â€œFemaleâ€ with 25.
### Prediction
- In this case, we divide our data set into two sets: One set with no missing values for the variable and another one with missing values. First data set become training data set of the model while second data set with missing values is test data set and variable with missing values is treated as target variable.
### Interpolation
### K-Nearest Neighbors Imputation
- KNN algorithm is very time-consuming in analyzing large database. It searches through all the dataset looking for the most similar instances.
- Choice of k-value is very critical. Higher value of k would include attributes which are significantly different from what we need whereas lower value of k implies missing out of significant attributes. 

# Outliers
## Types of Outliers
- Source: https://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/
- Univariate outliers can be found when we look at distribution of a single variable.
Multi-variate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.
### Data Entry Errors
- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.
### Data Processing Errors
- It is possible that some manipulation or extraction errors may lead to outliers in the dataset.
### Measurement Errors
- It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty. 
E## Experimental Errors
- For example: In a 100m sprint of 7 runners, one runner missed out on concentrating on the â€˜Goâ€™ call which caused him to start late. Hence, this caused the runnerâ€™s run time to be more than other runners.
### Intentional Outliers
- For example: Teens would typically under report the amount of alcohol that they consume. Only a fraction of them would report actual value. Here actual values might look like outliers because rest of the teens are under reporting the consumption.
### Sampling Errors
- For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample.
### Natural Outliers
## Outliers Detection
- Source: https://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/
## Outliers Treatment
### Deleting
### Transforming
- Natural log of a value reduces the variation caused by extreme values.
- Decision Tree algorithm allows to deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.
### Binning
- Decision Tree algorithm allows to deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.
### Imputing
- We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier.
### Treating Separately
- If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.
### Turkey Fences
- Source: https://cyan91.tistory.com/40, https://lsjsj92.tistory.com/556?category=853217
### Z-score
- Source: https://cyan91.tistory.com/40, https://soo-jjeong.tistory.com/121

# Sparse Respresentation, Dense Representation
- Source: https://dreamgonfly.github.io/blog/word2vec-explained/

# Anomaly

- ë‹¨ìˆœí•œ ë¬¸ì œëŠ” ë‹¨ìˆœí•œ ëª¨ë¸ë¡œ í’€ì–´ì•¼ í•¨
- ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ê´€ì¸¡ì¹˜ê°€ ë³€í•˜ëŠ” ë°ì´í„°

# Autoencoder
- Source: https://en.wikipedia.org/wiki/Autoencoder

# `graphviz`
```python
import graphviz
```
```python
def plot_tree(model, filename, rankdir="UT"):
    import os
    gviz = xgb.to_graphviz(model, num_trees = model.best_iteration, rankdir=rankdir)
    _, file_extension = os.path.splitext(filename)
    format = file_extension.strip(".").lower()
    data = gviz.pipe(format=format)
    full_filename = filename
    with open(full_filename, "wb") as f:
        f.write(data)
```

## `sklearn.feature_extraction.text`
### `CountVectorizer()`
```python
from sklearn.feature_extraction.text import CountVectorizer
```
```python
vect = CountVectorizer(max_df=500, min_df=5, max_features=500)
```
- Ignore if frequency of the token is greater than `max_df` or lower than `min_df`.
#### `vect.fit()`
#### `vect.transform()`
- Build document term maxtrix.
##### `vect.transform().toarray()`
#### `vect.fit_transform()`
- `vect.fit()` + `vect.transform()`
##### `vect.fit_transform().toarray()`
#### `vect.vocabulary_`
##### `vect.vocabulary_.get()`
- Return the index of the argument.
### `TfidfVectorizer()`
```python
from sklearn.feature_extraction.text import TfidfVectorizer
```
## `sklearn.preprocessing`
### `LabelEncoder()`
#### `le.fit()`, `le.transform()`, `le.fit_transform()`, `le.inverse_transform()`
#### `le.classes_`
```python
label2idx = dict(zip(le.classes_, set(label_train)))
```
### `StandardScaler()`, `MinMaxScaler()`, `RobustScaler()`, `Normalizer()`
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer
```
```python
sc = StandardScaler()
```
#### `sc.fit()`, `sc.transform()`, `sc.fit_transform()`
## `sklearn.decomposition`
### `PCA()`
```python
from sklearn.decomposition import PCA
```
```python
pca = PCA(n_components=2)
```
```python
pca_mat = pca.fit_transform(user_emb_df)
```
## `sklearn.pipeline`
### `Pipeline()`
```python
from sklearn.pipeline import Pipeline
```
```python
model = Pipeline([("vect", CountVectorizer()), ("model", SVC(kernel="poly", degree=8))])
```
- íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ê²°í•©ëœ ëª¨í˜•ì€ ì›ë˜ì˜ ëª¨í˜•ì´ ê°€ì§€ëŠ” fit, predict ë©”ì„œë“œë¥¼ ê°€ì§€ë©° ê° ë©”ì„œë“œê°€ í˜¸ì¶œë˜ë©´ ê·¸ì— ë”°ë¥¸ ì ì ˆí•œ ë©”ì„œë“œë¥¼ íŒŒì´í”„ë¼ì¸ì˜ ê° ê°ì²´ì— ëŒ€í•´ì„œ í˜¸ì¶œí•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ íŒŒì´í”„ë¼ì¸ì— ëŒ€í•´ fit ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ì „ì²˜ë¦¬ ê°ì²´ì—ëŠ” fit_transformì´ ë‚´ë¶€ì ìœ¼ë¡œ í˜¸ì¶œë˜ê³  ë¶„ë¥˜ ëª¨í˜•ì—ì„œëŠ” fit ë©”ì„œë“œê°€ í˜¸ì¶œëœë‹¤. íŒŒì´í”„ë¼ì¸ì— ëŒ€í•´ predict ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ì „ì²˜ë¦¬ ê°ì²´ì—ëŠ” transformì´ ë‚´ë¶€ì ìœ¼ë¡œ í˜¸ì¶œë˜ê³  ë¶„ë¥˜ ëª¨í˜•ì—ì„œëŠ” predict ë©”ì„œë“œê°€ í˜¸ì¶œëœë‹¤.
## `sklearn.svm`
### `SVC()`, `SVR()`
```python
from sklearn.svm import SVC
```
```python
SVC(kernel="linear")
```
- `kernel="linear"`
- `kernel="poly"`: gamma, coef0, degree
- `kernel="rbf"`: gamma
- `kernel="sigmoid"`: gomma, coef0
## `sklearn.naive_bayes`
```python
from sklearn.naive_bayes import MultinomialNB
```
## `sklearn.linear_model`
```python
from sklearn.linear_model import SGDClassifier
```
### `Ridge()`, `Lasso()`, `ElasticNet()`
```python
fit = Ridge(alpha=alpha, fit_intercept=True, normalize=True, random_state=123).fit(x, y)
```
#### `fit.intercept_`, `fit.coef_`
### `SGDClassifier`
```python
model = SGDClassifier(loss="perceptron", penalty="l2", alpha=1e-4, random_state=42, max_iter=100)
...
model.fit(train_x, train_y)
train_pred = model.pred(train_x)
train_acc = np.mean(train_pred == train_y)
```
- `loss`: The loss function to be used.
    - `loss="hinge"`: Give a linear SVM.
    - `loss="log"`: Give logistic regression.
    - `loss="perceptron"`: The linear loss used by the perceptron algorithm.
- `penalty`: Regularization term.
    - `penalty="l1"`
    - `penalty="l2"`: The standard regularizer for linear SVM models.
- `alpha`: Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when `learning_rate` is set to `"optimal"`.
- max_iter`: The maximum number of passes over the training data (aka epochs).
## `sklearn.ensemble`
### `RandomForestRegressor()`, `GradientBoostingRegressor()`, `AdaBoostRegressor()`
```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
```
## `sklearn.tree`
### `DecisionTreeRegressor()`
```python
from sklearn.tree import DecisionTreeRegressor
```
## `sklearn.datasets`
### `sklearn.datasets.fetch_20newsgroups()`
```python
newsdata = sklearn.datasets.fetch_20newsgroups(subset="train")
```
- `subset`: (`"all"`, `"train"`, `"test"`)
### `sklearn.datasets.sample_generator`
#### `make_blobs()`
```python
 from sklearn.datasets.sample_generator improt make_blobs
```
## `sklearn.metrcis`
### `sklearn.metrics.pairwise`
#### `sklearn.metrics.pairwise.cosine_similarity`
### `sklearn.metrics.classification_report()`
```python
print(sklearn.metrics.classification_report(y_pred, y_test))
```

# `tensorflow`
```python
import tensorflow as tf
from tensorflow.keras import Input, Model, Sequential
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator
from tensorflow.keras.layers import Layer, Dense, Flatten, Dropout, Concatenate, Add, Dot, Multiply, Reshape, Activation, BatchNormalization, SimpleRNNCell, RNN, SimpleRNN, LSTM, Embedding, Bidirectional, TimeDistributed, Conv1D, Conv2D, MaxPool1D, MaxPool2D, GlobalMaxPool1D, GlobalMaxPool2D, AveragePooling1D, AveragePooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, ZeroPadding2D
from tensorflow.keras.optimizers import SGD, Adam, Adagrad
from tensorflow.keras.metrics import RootMeanSquaredError, BinaryCrossentropy, SparseCategoricalAccuracy
from tensorflow.keras.layers.experimental.preprocessing import Rescaling
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.activations import linear, sigmoid, relu
```
## Create Tensors
### `tf.Variable(initial_value, [shape=None], [trainable=True], [validate_shape=True], [dtype], [name])`
- Source: https://www.tensorflow.org/api_docs/python/tf/Variable
- `initial_value`: This initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed.
- [`shape`]: The shape of this variable. If `None`, the shape of `initial_value` will be used.
- `validate_shape`: If `False`, allows the variable to be initialized with a value of unknown shape. If `True`, the default, the shape of `initial_value` must be known.
- [`dtype`]: If set, `initial_value` will be converted to the given type. If `None`, either the datatype will be kept (if `initial_value` is a Tensor), or `convert_to_tensor()` will decide.
### `tf.zeros()`
```python
W = tf.Variable(tf.zeros([2, 1], dtype=tf.float32), name="weight")
```
## Layers
## Computation between Tensors
### `tf.stack(values, axis, [name])`
- Source: https://www.tensorflow.org/api_docs/python/tf/stack
- Stacks a list of tensors of rank R into one tensor of rank (R + 1).
- `axis`: The axis to stack along.
- Same syntax as `np.stack()`
### `Add()`
```python
logits = Add()([logits_mlr, logits_fm, logits_dfm])
```
- It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).
### `Dot(axes)`
```python
pos_score = Dot(axes=(1, 1))([z1, z2])
```
- `axes` : (integer, tuple of integers) Axis or axes along which to take the dot product. If a tuple, should be two integers corresponding to the desired axis from the first input and the desired axis from the second input, respectively. Note that the size of the two selected axes must match.
### `Multiply()`
```python
def se_block(x, c, r):
	z = GlobalAveragePooling2D()(x)
	z = Dense(units=c//r, activation="relu")(z)
	z = Dense(units=c, activation="sigmoid")(z)
	z = Reshape(target_shape=(1, 1, c))(z)
	z = Multiply()([x, z])
	return z
```
## `Reshape()`
```python
z = Reshape(target_shape=(1, 1, ch))(z)
```
### `Concatenate()`
```python
Concatenate(axis=1)(embs_fm)
```
- tf.concat()ì™€ ë™ì¼í•©ë‹ˆë‹¤.
## `Activation()`
```python
x = Activation("relu")(x)
```
## Transforms Tensor Shape
### `Flatten()`
- ì…ë ¥ë˜ëŠ” tensorì˜ rowë¥¼ í¼ì³ì„œ ì¼ë ¬ë¡œ ë§Œë“­ë‹ˆë‹¤.
- í•™ìŠµë˜ëŠ” weightsëŠ” ì—†ê³  ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê¸°ë§Œ í•©ë‹ˆë‹¤.
```python
model.add(Flatten(input_shape=(28, 28)))
```
## `Input(shape, [name], [dtype], ...)`
- `shape`
	- ***A shape tuple (integers), not including the batch size***. For instance, shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors.
	- ***Elements of this tuple can be None; "None" elements represent dimensions where the shape is not known.***
## `Embedding(input_dim, output_dim, [mask_zero], [input_length], [name], ...)`
- Source: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding
- `input_dim`: Size of the vocabulary.
- `output_dim`: Dimension of the dense embedding.
- `input_length`: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream.
- `mask_zero=True`: Whether or not the input value 0 is a special "padding" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If `mask_zero` is set to `True`, as a consequence, index 0 cannot be used in the vocabulary (`input_dim` should equal to (Size of vocabulary + 1)).
- Input shape: `(batch_size, input_length)`
- Output shape: `(batch_size, input_length, output_dim)`
## `Dense()`
```python
Dense(units=52, input_shape=(13,), activation="relu")
```
- units: í•´ë‹¹ ì€ë‹‰ì¸µì—ì„œ í™œë™í•˜ëŠ” ë‰´ëŸ°ì˜ ìˆ˜(ì¶œë ¥ ê°’ì˜ í¬ê¸°)
- activation: í™œì„±í™”í•¨ìˆ˜, í•´ë‹¹ ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì˜ ì—°ì‚° ê²°ê³¼ë¥¼ ì–´ëŠ í•¨ìˆ˜ì— ì í•©í•˜ì—¬ ì¶œë ¥í•  ê²ƒì¸ê°€?
- input_shape : ì…ë ¥ ë²¡í„°ì˜ í¬ê¸°. ì—¬ê¸°ì„œ 13ì€ í•´ë‹¹ ë°ì´í„° í”„ë ˆì„ì˜ ì—´ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë°ì´í„°ì˜ êµ¬ì¡°(ì´ë¯¸ì§€, ì˜ìƒ)ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤. ì²« ë²ˆì§¸ ì€ë‹‰ì¸µì—ì„œë§Œ ì •ì˜í•´ì¤€ë‹¤.
## `Dropout()`
- rate : dropoutì„ ì ìš©í•  perceptronì˜ ë¹„ìœ¨
## `BatchNormalization()`
- usually used before activation function layers.
## `Conv1D()`
```python
Conv1D(filters=n_kernels, kernel_size=kernel_size, padding="same", activation="relu", strides=1)
```
- `strides` : basically equals to 1
## `Conv2D()`
```python
conv2d = Conv2D(filters=n_filters, kernel_size=kernel_size, strides=(1, 1), padding="same")(image)
```
- `image`: (batch, height of image, width of image, number of channels)
- `kernel`: (height of filter, width of filter, number of channels, number of kernels)
- `convolution`: (batch, height of convolution, width of convolution, number of kernels)
- number of channelsì™€ number of kernelsëŠ” ì„œë¡œ ë™ì¼í•©ë‹ˆë‹¤.
- `kernal_size`: window_size
- `padding="valid"`: No padding. There can be a loss of information. The size of the output image is smaller than the size of the input image.
- `padding="same"`: Normally, padding is set to same while training the model.
- `data_format`: (`"channels_last"`)
- `input_shape`: ì²˜ìŒì—ë§Œ ì„¤ì •í•´ ì£¼ë©´ ë©ë‹ˆë‹¤.
- `activation`: (`"tanh"`)
## Pooling Layers
### `MaxPool1D()`, `MaxPooling1D()`
- `strides` : basically equals to 2
### `MaxPool2D()`, `MaxPooling2D()`
```python
pool = MaxPool2D(pool_size=(2, 2), strides=1, padding="valid", data_format="channels_last")(image)
```
### `GlobalMaxPool1D()`, `GlobalMaxPooling1D()`
- Shape changes from (a, b, c, d) to (a, d).
### `GlobalMaxPool2D()`, `GlobalMaxPooling2D()`
- Downsamples the input representation by taking the maximum value over the time dimension.
- Shape changes from (a, b, c) to (b, c).
### `AveragePooling1D()`
### `AveragePooling2D([pool_size], [strides], [padding])`
### `GlobalAveragePooling1D()`
### `GlobalAveragePooling2D()`
### `ZeroPadding2D`
```python
z = ZeroPadding2D(padding=((1, 0), (1, 0)))(x)
```
- `padding`:
	- Int: the same symmetric padding is applied to height and width.
	- Tuple of 2 ints: interpreted as two different symmetric padding values for height and width: `(symmetric_height_pad, symmetric_width_pad)`.
	- Tuple of 2 tuples of 2 ints: interpreted as `((top_pad, bottom_pad), (left_pad, right_pad))`.
## `SimpleRNNCell()`
## `RNN()`
## `SimpleRNN()`, `GRU()`
```python
outputs, hidden_states = SimpleRNN(units=hidden_size)(x_data), input_shape=(timesteps, input_dim), return_sequences=True, return_state=True)(x_date)
```
- `SimpleRNN()` = `SimpleRNNCell()` + `RNN()`
- `batch_input_shape=(batch_size, timesteps, input_dim)`
- `return_sequences=False` : (default)time stepì˜ ë§ˆì§€ë§‰ì—ì„œë§Œ ì•„ì›ƒí’‹ì„ ì¶œë ¥í•©ë‹ˆë‹¤.(shape of output : (batch_size, hidden_size))
- `return_sequences=True` : ëª¨ë“  time stepì—ì„œ ì•„ì›ƒí’‹ì„ ì¶œë ¥í•©ë‹ˆë‹¤. many to many ë¬¸ì œë¥¼ í’€ê±°ë‚˜ LSTM ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ê°œë¡œ ìŒ“ì•„ì˜¬ë¦´ ë•ŒëŠ” ì´ ì˜µì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.(shape of output : (batch_size, timesteps, hidden_size))
- `return_state=True` : hidden stateë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.(shape of hidden state : (batch_size, hidden_size))
```python
GRU(units=hidden_size, input_shape=(timesteps, input_dim))
```
## `LSTM()`
```python
_, hidden_state, cell_state = LSTM(units=256, return_state=True)(inputs_enc)
```
- `tf.keras.layers.SimpleRNN()`ê³¼ ë¬¸ë²•ì´ ë™ì¼í•©ë‹ˆë‹¤.
- `return_state=True` : hidden stateì™€ cell stateë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
## `Bidirectional()`
```python
Bidirectional(tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True), input_shape=(timesteps, input_dim))
```
## `TimeDistributed()`
```python
model.add(TimeDistributed(tf.keras.layers.Dropout(rate=0.2)))
```
- TimeDistributedë¥¼ ì´ìš©í•˜ë©´ ê° timeì—ì„œ ì¶œë ¥ëœ ì•„ì›ƒí’‹ì„ ë‚´ë¶€ì— ì„ ì–¸í•´ì¤€ ë ˆì´ì–´ì™€ ì—°ê²°ì‹œì¼œì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
- In keras - while building a sequential model - usually the second dimension (one after sample dimension) - is related to a time dimension. This means that if for example, your data is 5-dim with (sample, time, width, length, channel) you could apply a convolutional layer using TimeDistributed (which is applicable to 4-dim with (sample, width, length, channel)) along a time dimension (applying the same layer to each time slice) in order to obtain 5-d output.
#### `tf.keras.layers.Layer`
- custom layerë¥¼ ë§Œë“¤ë ¤ë©´ `tf.keras.layers.Layer` í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ê³  ë‹¤ìŒ ë©”ì„œë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤
    - __init__: ì´ ì¸µì—ì„œ ì‚¬ìš©ë˜ëŠ” í•˜ìœ„ ì¸µì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. instance ìƒì„± ì‹œì— í˜¸ì¶œë©ë‹ˆë‹¤.
    - build: ì¸µì˜ ê°€ì¤‘ì¹˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. add_weight ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    - call: forward feeding ë‹¨ê³„ì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤. ì…ë ¥ ê°’ì„ ì´ìš©í•´ì„œ ê²°ê³¼ë¥¼ ê³„ì‚°í•œ í›„ ë°˜í™˜í•˜ë©´ ë©ë‹ˆë‹¤.

## `tf.identity()`
## `tf.constant()`
```python
image = tf.constant([[[[1], [2], [3]], [[4], [5], [6]], [[7], [8], [9]]]], dtype=tf.float32)
```
## `tf.convert_to_tensor()`
```python
img = tf.convert_to_tensor(img)
```
## `tf.transpose()`
## `tf.cast()`
```python
pred = tf.cast(h > 0.5, dtype=tf.float32)
```
- ì¡°ê±´ì´ Trueë©´ 1, Falseë©´ 0 ë°˜í™˜.
- í˜¹ì€ ë‹¨ìˆœíˆ Tensorì˜ ìë£Œí˜• ë³€í™˜.
## `tf.concat()`(= `tf.keras.layers.Concatenate()`)
```python
layer3 = tf.concat([layer1, layer2], axis=1)
```
- ì§€ì •í•œ axisì˜ dimensionì´ ìœ ì§€ë©ë‹ˆë‹¤.
## `tf.shape()`
```python
batch_size = tf.shape(conv_output)[0]
```
## `tf.reshape()`
```python
conv_output = tf.reshape(conv_output, shape=(batch_size, output_size, output_size, 3,
                                                 5 + n_clss))
```
## `tf.range()`
```python
tf.range(3, 18, 3)
```
## `tf.tile()`
```python
y = tf.tile(y, multiples=[1, output_size])
```
## `tf.constant_initializer()`
```
weight_init = tf.constant_initializer(weight)
```
## `tf.GradientTape()`
```python
with tf.GradientTape() as tape:
    hyp = W * X + b
    loss = tf.reduce_mean(tf.square(hyp - y))

dW, db = tape.gradient(loss, [W, b])
```
## `tf.math`
### `tf.math.add()`, `tf.math.subtract()`, `tf.math.multiply()`, `tf.math.divide()`
- Adds, substract, multiply or divide two input tensors element-wise.
### `tf.math.add_n(inputs)`
- Adds all input tensors element-wise.
- `inputs`: A list of Tensors, each with the same shape and type.
### `tf.math.square()`
- Compute square of x element-wise.
### `tf.math.argmax()`
```python
y_pred = tf.math.argmax(model.predict(X_test), axis=1)
```
### `tf.math.sign`
```python
tf.math.sign(tf.math.reduce_sum(self.w * x) + self.b)
```
### `tf.math.exp()`
### `tf.math.log()`
### `tf.math.equal()`
```python
acc = tf.math.reduce_mean(tf.cast(tf.math.equal(pred, labels), dtype=tf.float32))
```
### `tf.math.reduce_sum()`, `tf.math.reduce_mean()`
- Source: https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum#returns_1
- `axis=None`: ëª¨ë“  elementsì— ëŒ€í•´ ì—°ì‚°í•©ë‹ˆë‹¤.
- `axis=0`: reduces along the 1st dimension. dimensionì´ 1ë§Œí¼ ê°ì†Œí•©ë‹ˆë‹¤.
- `axis=1`: reduces along the 2nd dimension. dimensionì´ 1ë§Œí¼ ê°ì†Œí•©ë‹ˆë‹¤.
- `keepdims=True`: dimensionì´ ê°ì†Œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
## `tf.nn`
### `tf.nn.softmax()`
```python
h = tf.nn.softmax(tf.matmul(train_X, W) + b)
```
### `tf.nn.relu`
## `tf.data`
### `tf.data.Dataset`
#### `tf.data.Dataset.from_tensor_slices()`
```python
train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(len(train_x)).batch(batch_size, drop_remainder=True).prefetch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).shuffle(len(test_x)).batch(len(test_x)).prefetch(len(test_x))
```
##### `tf.data.Dataset.from_tensor_slices().shuffle()`
- ì§€ì •í•œ ê°œìˆ˜ì˜ ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ì–´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.
##### `tf.data.Dataset.from_tensor_slices().batch()`
- ì§€ì •í•œ ê°œìˆ˜ì˜ ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.
##### `tf.data.Dataset.from_tensor_slices().prefetch()`
- This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.
## `tf.train`
### `tf.train.Checkpoint()`
## `tf.keras`
### `Sequential()`
```python
model = Sequential()
```

## Build Model
```python
model = Model(inputs, ouputs, [name])
```
## Compile
```python
# `optimizer`: (`"sgd"`, `"adam"`, `"rmsprop"`)
# `loss`: (`"mse"`, `"binary_crossentropy"`, `"categorical_crossentropy"`, `"sparse_categorical_crossentropy"`)
# `metrics`: (`["mse"]`, `["binary_accuracy"]`, `["categorical_accuracy"]`, `["sparse_categorical_crossentropy"]`, `["acc"]`)
# `loss_weights`
model.compile(optimizer, loss, metrics, [loss_weights])

model.summary()
```
## Fit
- Source: https://keras.io/api/models/model_training_apis/
```python
# `mode`: (`"auto"`, `"min"`, `"max"`).
	# `"min"`: Training will stop when the quantity monitored has stopped decreasing;
	# `"max"`: It will stop when the quantity monitored has stopped increasing;
	# `"auto"`: The direction is automatically inferred from the name of the monitored quantity.
# `patience`: Number of epochs with no improvement after which training will be stopped.
es = EarlyStopping(monitor="val_loss", mode="auto", verbose, patience)
model_path = "model_path.h5"
# `verbose=1`: ëª¨ë¸ì´ ì €ì¥ ë  ë•Œ 'ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤' ë¼ê³  í™”ë©´ì— í‘œì‹œë©ë‹ˆë‹¤.
# `save_best_only=True`: `monitor` ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ì¢‹ì€ ê°’ìœ¼ë¡œ ëª¨ë¸ì´ ì €ì¥ë©ë‹ˆë‹¤.
# `save_best_only=False`: ë§¤ epochë§ˆë‹¤ ëª¨ë¸ì´ filepath{epoch}ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
# `save_weights_only=True`: ëª¨ë¸ì˜ weightsë§Œ ì €ì¥ë©ë‹ˆë‹¤.
# `save_weights_only=False`: ëª¨ë¸ ë ˆì´ì–´ ë° weights ëª¨ë‘ ì €ì¥ë©ë‹ˆë‹¤.
mc = ModelCheckpoint(filepath=model_path, monitor="val_acc", mode, verbose=1, [save_best_only])
# `x`
# `y`
# `validation_split`
# `verbose=2`: One line per epoch. recommended.
hist = model.fit(x=X, y=y, validation_split=0.2, batch_size, epochs, verbose=2, shuffle=True, callbacks=[es, mc])
```
```python
hist = model.fit(train_ds, validation_data=val_ds, epochs=epochs)
```
```python
hist = model.fit_generator(generator=train_set.shuffle(len(x_train)).batch(batch_size), epochs=n_epochs, validation_data=val_set.batch(batch_size))
```
### `hist.history`
```python
hist.history["accuracy"]
```
- (`"accuracy"`, `"val_accuracy"`, `"loss"`, `"val_loss"`)
## Evaluate Model
```python
score = model.evaluate(x_test, y_test, batch_size=128, verbose=0)
```
## Predict
```python
preds = model.predict(x.values)
```
## ê°€ì¤‘ì¹˜ í™•ì¸
```python
for layer in model.layers:
	...
```
```python
layer = model.get_layer("layer_name")

name = layer.name
output = layer.output
input_shape = layer.input_shape
output_shape = layer.output_shape
weight = layer.get_weights()[0]
bias = layer.get_weights()[1]
```
#### `model.trainable_variables`
#### `model.save()`
#### `model.input`

### `tf.keras.utils`
#### `tf.keras.utils.get_file()`
```python
base_url = "https://pai-datasets.s3.ap-northeast-2.amazonaws.com/recommender_systems/movielens/datasets/"
movies_path = tf.keras.utils.get_file(fname="movies.csv", origin=os.path.join(base_url, "movies.csv"))

movie_df = pd.read_csv(movies_path)
```
- ì¸í„°ë„·ì˜ íŒŒì¼ì„ ë¡œì»¬ ì»´í“¨í„°ì˜ í™ˆ ë””ë ‰í† ë¦¬ ì•„ë˜ `.keras/datasets` ë””ë ‰í† ë¦¬ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
- `untar=True`
### `tf.keras.backend`
#### `tf.keras.backend.clear_session()`
- Resets all state generated by Keras.
#### `optimizer.apply_gradients()`
```python
opt.apply_gradients(zip([dW, db], [W, b]))
```
```python
opt.apply_gradients(zip(grads, model.trainable_variables))
```
### `tf.keras.losses`
#### `tf.keras.losses.MeanSquaredError()`(= `"mse"`)
#### `tf.keras.losses.BinaryCrossentropy()`(= `"binary_crossentropy"`)
#### `tf.keras.losses.categorical_crossentropy()`
- Source: [https://hwiyong.tistory.com/335](https://hwiyong.tistory.com/335)
- ë”¥ëŸ¬ë‹ì—ì„œ ì“°ì´ëŠ” logitì€ ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì¶œë ¥ê°’ì´ ë¬¸ì œì— ë§ê²Œ normalize ë˜ì—ˆëŠëƒì˜ ì—¬ë¶€ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 10ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œì—ì„œëŠ” ì£¼ë¡œ softmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°ìš”. ì´ë•Œ, ëª¨ë¸ì´ ì¶œë ¥ê°’ìœ¼ë¡œ í•´ë‹¹ í´ë˜ìŠ¤ì˜ ë²”ìœ„ì—ì„œì˜ í™•ë¥ ì„ ì¶œë ¥í•œë‹¤ë©´, ì´ë¥¼ logit=Falseë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì´ê±´ ì €ë§Œì˜ í‘œí˜„ì¸ ì ì„ ì°¸ê³ í•´ì„œ ì½ì–´ì£¼ì„¸ìš”). ë°˜ëŒ€ë¡œ ëª¨ë¸ì˜ ì¶œë ¥ê°’ì´ sigmoid ë˜ëŠ” linearë¥¼ ê±°ì³ì„œ ë§Œë“¤ì–´ì§€ê²Œ ëœë‹¤ë©´, logit=Trueë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì—ì„œ softmax í•¨ìˆ˜ë¥¼ ê±°ì¹˜ë©´ `from_logits=False`(defaultê°’), ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ `from_logits=True`(numerically stable)
- ì •ë‹µ ë ˆì´ë¸”ì´ one-hot encoding í˜•íƒœì¼ ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤.
#### `tf.keras.losses.sparse_categorical_crossentropy()`
```python
def loss_fn(model, x, y):
    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=model(x), from_logits=True))
```
- ì •ë‹µ ë ˆì´ë¸”ì´ one-hot vectorê°€ ì•„ë‹ ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤.
##### `tf.keras.layers.experimental.preprocessing`
###### `Rescaling`
```python
model.add(Rescaling(1/255, input_shape=(img_height, img_width, 3)))
```
```python
from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom
```
###### `RandomFlip`
```python
data_aug.add(RandomFlip("horizontal", input_shape=(img_height, img_width, 3)))
```
###### `RandomRotation`
```python
data_aug.add(RandomRotation(0.1))
```
###### `RandomZoom()`
```python
data_aug.add(RandomZoom(0.1))
```
```python
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
```
### `tf.keras.initializers`
#### `tf.keras.initializers.RandomNormal()`
#### `tf.keras.initializers.glorot_uniform()`
#### `tf.keras.initializers.he_uniform()`
#### `tf.keras.initializers.Constant()`
### `tf.keras.preprocessing`
#### `tf.keras.preprocessing.image`
##### `load_img()`
```python
from tensorflow.keras.preprocessing.image import load_img
```
```python
img = load_img(fpath, target_size=(img_height, img_width))
```
##### `img_to_array()`
```python
from tensorflow.keras.preprocessing.image import img_to_array
```
```python
img_array = img_to_array(img)
```
#### `image_dataset_from_directory()`
```python
from tf.keras.preprocessing import image_dataset_from_directory
```
```python
train_ds = image_dataset_from_directory(data_dir, validation_split=0.2, subset="training",
                                        image_size=(img_height, img_width), seed=1, batch_size=batch_size)
```
```python
for image_batch, labels_batch in train_ds:
    print(image_batch.shape)
    print(labels_batch.shape)
    break
```
##### `ds.class_names`
```python
train_ds.class_names
```
##### `ds.take()`
```python
plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        ax.imshow(images[i].numpy().astype("uint8"))
        ax.set_title(cls_names[labels[i]])
        ax.axis("off")
```
##### `ImageDataGenerator`
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator
```
```python
gen = ImageDataGenerator(rescale=1/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
```
- `validation_split`
- `shear_range` : float. Shear Intensity (Shear angle in counter-clockwise direction as radians)
- `zoom_range` : Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]
- `horizontal_flip` : Boolean. Randomly flip inputs horizontally.
- `rescale` : rescaling factor. Defaults to None. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (before applying any other transformation).
- `rotation_range`
- `width_shift_range`: (Float) Fraction of total width if < 1 or pixels if >= 1. (1-D Array-like) Random elements from the Array. (Int) Pixels from interval (-`width_shift_range`, `width_shift_range`)
- `height_shift_range`
- `brightness_range` : Tuple or List of two Floats. Range for picking a brightness shift value from.
- `zoom_range`
- `horizontal_flip`
- `vertical_flip`
- transformationì€ ì´ë¯¸ì§€ì— ë³€í™”ë¥¼ ì£¼ì–´ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ë§ê²Œ í•´ì„œ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— train setë§Œ í•´ì£¼ê³ , test setì—ëŠ” í•´ ì¤„ í•„ìš”ê°€ ì—†ë‹¤. ê·¸ëŸ¬ë‚˜ ì£¼ì˜í•  ê²ƒì€ Rescaleì€ train, test ëª¨ë‘ í•´ ì£¼ì–´ì•¼ í•œë‹¤.
- References: https://m.blog.naver.com/PostView.nhn?blogId=isu112600&logNo=221582003889&proxyReferer=https:%2F%2Fwww.google.com%2F
###### `gen.fit()`
- Only required if `featurewise_center` or `featurewise_std_normalization` or `zca_whitening` are set to True.
###### `gen.flow()`
```python
hist = model.fit(gen.flow(x_tr, y_tr, batch_size=32), validation_data=gen.flow(x_val, y_val, batch_size=32),
                 epochs=10)
```
###### `gen.flow_from_directory()`
```python
gen = ImageDataGenerator()
datagen_tr = gen.flow_from_directory(directory="./dogsandcats", target_size=(224, 224))
```
- `batch_size=batch_size`
- `target_size`: the dimensions to which all images found will be resized.
- `class_mode`: (`"binary"`, `"categorical"`, `"sparse"`, `"input"`, `None`)
- `class_mode="binary"`: for binary classification.
- `class_mode="categorical"`: for multi-class classification(OHE).
- `class_mode="sparse"`: for multi-class classification(no OHE).
- `class_mode="input"`
- `class_mode=None`: Returns no label.
- `subset`: (`"training"`, `"validation"`) Subset of data if `validation_split` is set in ImageDataGenerator().
- `shuffle`
#### `tf.keras.preprocessing.sequence`
##### `pad_sequences()`
```python
train_X = pad_sequences(train_X, maxlen=max_len)
```
```python
X_char = [pad_sequences([[char2idx[char] if char in chars else 1 for char in word] for word in sent]) for sent in corpus]
```
```python
train_X = pad_sequences([tokenizer.convert_tokens_to_ids(tokens) for tokens in tokens_lists], 
                        maxlen=max_len, value=tokenizer.convert_tokens_to_ids("[PAD]"),
                        truncating="post", padding="post")
```
- `padding`: (`"pre"`, `"post"`)
- `truncating`: (`"pre"`, `"post"`)
- `value=` : paddingì— ì‚¬ìš©í•  valueë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
#### `tf.keras.preprocessing.text`
##### `tf.keras.preprocessing.text.Tokenizer()`
```python
tkn = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size+2, oov_token="UNK", lower=True)
```
- `lower=False`: ëŒ€ë¬¸ìë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
##### `tkn.fit_on_texts()`
```python
tkn.fit_on_texts(["ë‚˜ë‘ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°ˆë˜ ì ì‹¬ ë©”ë‰´ëŠ” í–„ë²„ê±° ê°ˆë˜ ê°ˆë˜ í–„ë²„ê±° ìµœê³ ì•¼"])
```
##### `tkn.word_index`
```python
word2idx = tkn.word_index
```
##### `tkn.index_word`
##### `tkn.word_counts`
```python
word2cnt = dict(sorted(tkn.word_counts.items(), key=lambda x:x[1], reverse=True))

cnts = list(word2cnt.values())

for vocab_size, value in enumerate(np.cumsum(cnts)/np.sum(cnts)):
    if value >= ratio:
        break

print(f"{vocab_size:,}ê°œì˜ ë‹¨ì–´ë¡œ ì „ì²´ dataì˜ {ratio:.0%}ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
print(f"{len(word2idx):,}ê°œì˜ ë‹¨ì–´ ì¤‘ {vocab_size/len(word2idx):.1%}ì— í•´ë‹¹í•©ë‹ˆë‹¤.")
```
##### `tkn.texts_to_sequences()`
```python
train_X = tkn.texts_to_sequences(train_X)
```
- `num_words`ê°€ ì ìš©ë©ë‹ˆë‹¤.
##### `tkn.sequences_to_texts()`
##### `tkn.texts_to_matrix()`
```python
tkn.texts_to_matrix(["ë¨¹ê³  ì‹¶ì€ ì‚¬ê³¼", "ë¨¹ê³  ì‹¶ì€ ë°”ë‚˜ë‚˜", "ê¸¸ê³  ë…¸ë€ ë°”ë‚˜ë‚˜ ë°”ë‚˜ë‚˜", "ì €ëŠ” ê³¼ì¼ì´ ì¢‹ì•„ìš”"], mode="count"))
```
- `mode`: (`"count"`, `"binary"`, `"tfidf"`, `"freq"`)
- `num_words`ê°€ ì ìš©ë©ë‹ˆë‹¤.
### `tf.keras.models`
#### `tf.keras.models.load_model()`
```python
model = tf.keras.models.load_model(model_path)
```
### `tf.keras.applications`
#### `tf.keras.applications.VGG16()`
```python
vgg = tf.keras.applications.VGG16(input_shape=(224, 224, 3), include_top=False, weights="imagenet")
```
##### `vgg.trainable`
```python
vgg.trainable=Flase
```

# `tensorflow_addons`
```python
import tensorflow_addons as tfa
```
## `tfa.optimizers`
### `tfa.optimizers.RectifiedAdam()`
```python
opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)
```

# `tensorflow_hub`
```python
import tensorflow_hub as hub
```
## `hub.Module()`
```python
elmo = hub.Module("https://tfhub.dev/google/elmo/3", trainable=True)
```
### `elmo()`
```python
embeddings = elmo(["the cat is on the mat", "dogs are in the fog"], signature="default", as_dict=True)["elmo"]
```

# `tensorflow_datasets`
```python
import tensorflow_datasets as tfds
```
## `tfds.deprecated`
### `tfds.deprecated.text`
#### `tfds.deprecated.text.SubwordTextEncoder`
##### `tfds.deprecated.text.SubwordTextEncoder.build_from_corpus()`
```python
tkn = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_data["document"], target_vocab_size=2**13)
```
##### `tkn.subwords`
##### `tkn.encode()`
```python
tkn.encode(train_data["document"][20])
```
##### `tkn.decode()`
```python
tkn.decode(tkn.encode(sample))
```

# `torch`
```python
conda install pytorch cpuonly -c pytorch
```
```python
import torch
```
## `torch.tensor()`
## `torch.empty()`
## `torch.ones()`
```python
w = torch.ones(size=(1,), requires_grad=True)
```
- Returns a tensor filled with the scalar value `1`, with the shape defined by the variable argument `size`
- `requires_grad`: (bool)
## `torch.ones_like()`
```python
torch.ones_like(input()
```
- `input`: (Tensor)
- Returns a tensor filled with the scalar value `1`, with the same size as `input`.
### `Tensor.data`
### `Tensor.shape`
### `Tensor.size()`
### `Tensor.view()`
```python
torch.randn(4, 4).view(-1, 8)
```
- Returns a new tensor with the same data as the `self` tensor but of a different `shape`.
### `Tensor.float()`
### `Tensor.backward()`
### `Tensor.grad`
- ë¯¸ë¶„í•  Tensorì— ëŒ€í•´ `requires=False`ì´ê±°ë‚˜ ë¯¸ë¶„ë  Tensorì— ëŒ€í•´ `Tensor.backward()`ê°€ ì„ ì–¸ë˜ì§€ ì•Šìœ¼ë©´ `None`ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
## `torch.optim`
### `torch.optim.SGD()`, `torch.optim.Adam()`
```python
opt = torch.optim.SGD(params=linear.parameters(), lr=0.01)
```
#### `opt.zero_grad()`
- Sets the gradients of all optimized Tensors to zero.
#### `opt.step()`
## `torch.nn`
```python
import torch.nn as nn
```
### `nn.Linear()`
```python
linear = nn.Linear(in_features=10, out_features=1)
```
- `in_features`: size of each input sample.
- `out_features`: size of each output sample.
### `linear.parameters()`
```python
for param in linear.parameters():
    print(param)
    print(param.shape)
    print('\n')
```
### `linear.weight`, `linear.bias`
#### `linear.weight.data`, `linear.bias.data`
## `nn.MSELoss()`
```python
loss = nn.MSELoss()(ys_hat, ys)
```
### `loss.backward()`
## `nn.Module`
## `nn.ModuleList()`

# `xgboost`
```python
import xgboost as xgb
```
## `xgb.DMatrix()`
```python
dtrain = xgb.DMatrix(data=train_X, label=train_y, missing=-1, nthread=-1)
dtest = xgb.DMatrix(data=test_X, label=test_y, missing=-1, nthread=-1)
```
## `xgb.train()`
```python
params={"eta":0.02, "max_depth":6, "min_child_weight":5, "gamma":1, "subsample":0.5, "colsample_bytree":1, "reg_alpha":0.1, "n_jobs":6}
watchlist = [(dtrain, "train"), (dtest,"val")]
num=12
def objective(pred, dtrain):
    observed = dtrain.get_label()
    grad = np.power(pred - observed, num - 1)
    hess = np.power(pred - observed, num - 2)
    return grad, hess
def metric(pred, dtrain):
    observed = dtrain.get_label()
    return "error", (pred - observed)/(len(observed), 1/num)
model = xgb.train(params=params, evals=watchlist, dtrain=dtrain, num_boost_round=1000, early_stopping_rounds=10, obj=objective, feval=metric)
```
## `xgb.XGBRegressor()`
```python
model = xgb.XGBRegressor(booster="gbtree", max_delta_step=0, importance_type="gain", missing=-1, n_jobs=5, reg_lambda=1, scale_pos_weight=1, seed=None, base_score=0.5, verbosity=1, warning="ignore", silent=0)
model.eta=0.01
model.n_estimators=1000
model.max_depth=6
model.min_child_weight=5
model.gamma=1
model.subsample=0.5
model.colsample_bytree=1
model.reg_alpha=0.1
model.objective = custom_se
model.n_jobs=5
```
- `n_estimators`
### `model.fit()`
```python
model.fit(train_X, train_y, eval_set=[(train_X, train_y), (val_X, val_y)], early_stopping_rounds=50, verbose=True)
```

# `seqeval`
## `seqeval.metrics`
### `precision_score`
### `recall_score`
### `f1_score`
### `classification_report`
```python
from seqeval.metrics import classification_report
```